---
title: The Feasibility and Equity of Geolocation Data for Lapse Prediction in AUD
author:
    #  - Investigation
  - name: Claire Punturieri 
    email: punturieri@wisc.edu
    #orcid: 0009-0003-7743-3736
    #corresponding: false
    #roles: []
    #affiliations:
      #- Department of Psychology, University of Wisconsin-Madison
  - name: John J. Curtin 
    #orcid: 0000-0002-3286-938X
    #corresponding: true
    email: jjcurtin@wisc.edu
    #roles:
      #- Project administration
      #- Software
      #- Visualization
    #affiliations:
      #- Department of Psychology, University of Wisconsin-Madison 
#keywords:
  #- Substance use disorders
  #- Precision mental health 
#abstract: |
  #To be filled in.
#plain-language-summary: |
  #To be filled in.
#key-points:
  #- Take away point 1 
  #- Take away point 2
date: last-modified
bibliography: references.bib
#citation:
  #container-title: To be filled in. 
number-sections: false 
editor_options: 
  chunk_output_type: console
---

## Introduction

About 1 in 10 adults in the United States met diagnostic criteria for alcohol use disorder (AUD) in 2022 [@Highlights2022National]. While some individuals will experience natural recovery (i.e., improvement without intervention) [@tuckerEpidemiologyRecoveryAlcohol2020], for others AUD will present as a chronic, relapsing disorder marked by periods of recovery interspersed with returns back to harmful use [@ScienceDrugUse; @brandonRelapseRelapsePrevention2007a]. For such individuals, continued monitoring may be beneficial in assisting with the maintenance of recovery goals and in identifying precipitants to lapses, or single instances of goal-inconsistent use that may lead to relapse [@witkiewitzRelapsePreventionAlcohol2004a]. One sustainable and scalable way to provide this continuous monitoring to individuals who need it most is through developing algorithms to predict lapses using both personal sensing data and machine learning.

Personal sensing data are data derived via embedded sensors in technology such as smartphones, smartwatches, or wearables [@mohrPersonalSensingUnderstanding2017a]. Because these devices are already ubiquitous within our day-to-day lives, one benefit of porting these data to clinical use is their proven ability to be collected unobtrusively and continuously. Importantly, these data do not require individuals to change their behavior or routines in any way. Moreover, when paired with machine learning models, statistical patterns connecting antecedents to lapse derived from these data (e.g., changes in mood, difficulty with close social connections, proximity to risky locations) to true lapse events can be uncovered. This is crucial for several reasons: 1) even when someone anticipates an oncoming lapse, it may be difficult to pinpoint the specific driving forces behind it; 2) these precipitating factors will have great variation both between- and within-people; and 3) uncovering these factors may help relieve some of the cognitive burden of recovery (i.e., constant monitoring of potential environmental risk factors).

### Geolocation Data for Risk Monitoring

Recovery and return to use are dynamic processes. Factors that contribute both to maintenance of recovery and return to use change from person-to-person and from moment-to-moment. A shift in social supports (e.g., a move, a break-up) may precede a lapse for one individual but not another. Time spent in locations where alcohol is available (e.g., bars, restaurants, concert venues) may precede a given lapse for another individual, but will not necessarily precede future lapses in that same individual. In order to best capture this fluidity, the ideal data type used within continuous risk monitoring systems should be able to provide a correspondingly appropriate level of granularity. Geolocation data are one such promising source.

Geolocation data consist of latitude and longitude coordinates and can be sampled at regular intervals using applications on smartphones with little to no input from the user beyond initial set-up. Many smartphones and smartwatches automatically collect these data by default. This fact, paired with increasing rates of smartphone ownership, suggest that there is high potential for these data to be feasibly harnessed for use in a risk-monitoring system [@areanMobileTechnologyMental2016]. The importance of location, such as environmental cues or one's perceived riskiness of a setting, has been shown to play an important role in lapse [@janakPotentEffectEnvironmental2010; @waltonIndividualSocialEnvironmental2003;  @waltonSocialSettingsAddiction1995]. This link with lapse risk has translated into the integration of coping skills that target substance-associated contexts in several treatment strategies like mindfulness-based relapse prevention [@lecocqConsideringDrugAssociatedContexts2020]. These findings underscore not only the potential wealth of information relating to lapse risk that an individual's location can provide, but also demonstrate the proven integration of location information into treatment. Furthermore, geolocation data have been specifically identified as being of particular use in both understanding the precipitants to harmful substance use and its effective treatment [@stahlerGeospatialTechnologyExposome2013].

Within the substance use literature, geolocation data have historically been used to examine risky locations, such as the influence of neighborhood characteristics on use [@epsteinRealtimeTrackingNeighborhood2014a; @kwanUncertaintiesGeographicContext2019] and individual physical proximity to locations of potential or past harmful use such as bars (either estimated using geofencing or user-defined) [@attwoodUsingMobileHealth2017; @carreiroRealizeAnalyzeEngage2021; @gonzalezComparisonSmartphoneApp2015; @gustafsonSmartphoneApplicationSupport2014a; @naughtonContextSensingMobilePhone2016]. Several of the applications implemented in these studies enable real-time notifications about locations to their users (e.g., a pop-up message on a smartphone which reads *"You are entering a high-risk zone"*).

On the other hand, affective scientists have focused instead more closely on factors relating to mood. Geolocation data have been used to estimate loneliness and isolation [@doryabIdentifyingBehavioralPhenotypes2019a], to demonstrate increases in positive affect from seeking out novel environments [@hellerAssociationRealworldExperiential2020], and to quantify depressive symptoms [@raughGeolocationDigitalPhenotyping2020]. Moreover, these data have not only been harnessed to measure mood symptoms, but to also predict their emergence [for review, see @shinSystematicReviewLocation2023].

An integration across these subfields can be in part accomplished by enriching geolocation data with brief, intermittent surveys probing specific information about frequently visited locations. For example, some of the more nuanced facets captured within location are associations with others (or lack thereof, e.g., social isolation), associations with previous drinking behaviors (e.g., whether or not alcohol is present), and associations with affect (i.e., negative versus positive emotions tied to a given location).

### Model Evaluation

Data selection, however, is only one component of the successful development of a continuous risk monitoring algorithm. Following development, it is imperative that these models be rigorously evaluated using performance metrics and eventually tested using independent observations (i.e., using the model to predict outcomes for individuals whose data were not used in development). This workflow in machine learning is what enables researchers to anticipate how well a model could be expected to generalize to new populations and is key when aiming to develop algorithms for real-world healthcare implementation. While standard performance metrics like model accuracy, for example, have been standard reporting practice for years, recent literature has begun to urge researchers to also include assessments of how *fair* a model is [@rajkomarEnsuringFairnessMachine2018a; @wawiragichoyaEquityEssenceCall2021]. A fair algorithm is one with no preference in performance with respect to inherent or acquired characteristics (e.g., gender, race, socioeconomic status; [@wangBriefReviewAlgorithmic2022]). In the context of a continuous risk monitoring algorithm for AUD, this would mean that lapse predictions are reasonably accurate and do not favor or disadvantage any particular group solely due to group membership status.

The motivating factors behind this call to action are clear. In the broader context of health-related data, historical patterns of health care inequities will almost certainly and unavoidably be embedded within data used to train algorithms. These inequities may unintentionally be carried forward, and maybe exacerbated, by machine learning models if not critically examined. Without a careful eye towards these foreseeable consequences, monitoring algorithms run the risk of providing sub-optimal mental health care to individuals who already face disadvantages.

There are at least two clear pathways whereby algorithmic performance may diverge between subgroups. The first is non-representative sampling. During model training, an algorithm learns to associate patterns in observations with a given outcome. Model performance will therefore suffer if there is limited information from which to learn (e.g., few instances of a given demographic trait), particularly without the use of resampling techniques to amend these imbalances [@japkowiczClassImbalanceProblem2000; @wangIntersectionalityMachineLearning2022].

The second is historical biases in the literature. This is of particular concern specifically in the context of AUD,  where the literature has historically been built upon research developed with male, predominantly white, participants. Despite the call to action brought forth by the NIH through their *Guidelines on Inclusion of Women and Minorities in Research*, recent work has highlighted that seminal research in the field on medications for the treatment of AUD have failed to consistently report participant demographics [@schickCallActionSystematic2020]. This lack of reporting makes it difficult to assess how and if this lack of representation is being corrected. By the very nature of its historically limited participant pool, AUD research and its theory have been developed from a particular perspective using a particular group of individuals. This means that the variables that researchers decide are important to measure and input into models, informed by knowledge of AUD theory, will inherently be biased and may favor these groups. Therefore, researchers can also not assume that balanced classes are enough to compensate for biases brought on by the broader societal context. Both of these facts motivate the reasoning behind examining algorithmic fairness in the context of developing continuous risk monitoring systems.

### The Current Study

In order for these continuous risk monitoring systems to be implemented in the real-world, these models must both be developed outright and rigorously evaluated on both standard performance metrics and for algorithmic fairness. To this end, this study utilized geolocation data collected from smartphones and corresponding self-reported contextual information for frequently visited locations to build a machine learning model to predict next-day alcohol use lapse among individuals with a diagnosis of AUD and a recovery goal of abstinence. Model features were engineered from both raw geolocation data and change in these data over the previous 6, 12, 24, 48, 72, and 168 hours.

Here we present characterization of model performance for this prediction model in a validation set, including a discussion of feature importance and an evaluation of model fairness. This study constitutes a preliminary evaluation of a model designed to predict lapse back to alcohol use using minimally burdensome data that has the potential to be integrated within a continuous risk monitoring platform.

## Methods

### Participants

One hundred and forty six individuals in early-recovery (1-8 weeks of abstinence) for AUD were recruited from the Madison area to take part in a three-month study on how mobile health technology can provide recovery support between 2017 and 2019 (R01 AA024391). Recruitment approaches included social media platforms (e.g., Facebook), television and radio advertisements, and clinic referrals. Prospective participants completed a phone screen to assess match with eligibility criteria (@tbl-elig). Participants were excluded if they exhibited severe symptoms of paranoia or psychosis (a score \<= 2.24 on the SCL-90 psychosis scale or a score \<= 2.82 on the SCL-90 paranoia scale administered at screening). Participants completed a baseline measure of demographics and other constructs relevant to lapse at the screening visit, which was used for fairness assessments (@tbl-demo-1).

### Procedure

Participants enrolled in a three-month study consisting of five in-person visits, daily surveys, and continuous passive monitoring of geolocation data. Following screening and enrollment visits in which participants consented to participate, learned how to manage location sharing (i.e., turn off location sharing when desired), and reported frequently visited locations, participants completed three follow-up visits one month apart. At each visit, participants were asked questions about frequently visited (\>2 times during the course of the previous month) locations (@tbl-context). Participants were debriefed at the third and final follow-up visit. Participants were expected to provide continuous geolocation data while on study. Other personal sensing data streams (EMA, cellular communications, sleep quality, and audio check-ins) were collected as part of the parent grant’s aims (R01 AA024391).

### Geolocation and contextual data

To enable collection of geolocation data, participants downloaded either the Moves app or the FollowMee app during the intake visit. Moves was bought-out and subsequently deprecated while the study was ongoing (July 2018) and data collection continued using FollowMee until the end of the study. Both apps continuously tracked location via GPS and WiFi positioning technology.

After completion of the study, data were processed to filter out duplicated points, fast movement speeds (\>100mph), sudden positional jumps, and periods of long duration suggesting sampling error issues (\>24 hours with no movement or \>2 hours with a positional jump of more than 0.31 miles or 500 meters). Data points were classified as "in transit" when spacing between individual positions suggested a movement speed of greater than 4mph per NIH health guidelines [@PhysicalActivityGuidelines]. Participants were considered to be at a known contextual location if they were within 0.031 miles (50 meters) of a reported frequently visited location.

### Data analytic strategy

Data preprocessing, modeling, and Bayesian analyses were done in R using the tidymodels ecosystem [@kuhnTidymodelsCollectionPackages2020]. Models were trained using high-throughput computing resources provided by the University of Wisconsin Center for High Throughput Computing [@chtc].

### Lapses

Alcohol lapses were used as the outcome variable in this study and were used to provide labels for model training, for testing model performance, and for testing issues of algorithmic fairness. Future lapse occurrence (here conceptualized as next-day lapse) was predicted in 24-hour windows, beginning at 4:00am on a participant's second day of participation to ensure one full day of data collection for the first window, and at every subsequent day on study thereafter. *Lapse* and *no lapse* occurrences were identified from the daily survey question, *"Have you drank any alcohol that you have not yet reported?"*. Participants who responded *yes* to this question were then asked to report the date and hour of the start and the end of the drinking episode. In this case, the prediction window was labeled *lapse*. Prediction windows were labeled *no lapse* if no alcohol use was reported within that window.

### Feature engineering

Feature engineering is the process of creating variables (or *"features"*) from unprocessed data and was used to transform raw data from geolocation data collected the prior day. Separate feature categories were created for the six contextual geolocation categories (presented in @tbl-context) and for three movement-based categories: variability in location, time spent outside of the home in the evening, and time spent in transit. All features were calculated both as raw and change features based on previous geolocation data (i.e., change from past 6, 12, 24, 48, 72, and 168 hour periods) in order to capture individual variation.

Imputation of missing data and removal of zero-variance features are additional general processing steps that were completed during feature engineering.

### Algorithm development & performance

We trained and assessed several configurations of an XGBoost machine learning algorithm. The choice of using an XGBoost algorithm was motivated by two main reasons: 1) the calculation of Shapley values, used to understand the relative contributions of features in predictions, is optimized for XGBoost; and 2) previous work in our lab has made use of XGBoost algorithms in model development [@wyantMachineLearningModels2023] and the ability to eventually integrate features across models is of high priority. Configurations of the XGBoost algorithm varied across a relevant and appropriate range of model-specific hyperparameters (mtry, tree depth, learning rate) as well as resampling techniques (up-sampling of the positive class, lapse, and down-sampling of the negative class, no lapse ranging from 1:1 to 5:1) to account for the class imbalance in our outcome variable.

Models were trained and assessed using participant-grouped, nested *k*-fold cross-validation. Grouped cross-validation ensures that all data from a given participant are retained as either held-in or held-out. This prevents the introduction of bias from a participant's data being used to predict their own data. Nested cross-validation uses two nested loops for dividing and holding out folds: an outer loop, where held-out folds serve as test sets for model evaluation; and inner loops, where held-out folds serve as validation sets for model selection [@jonathanUseCrossvalidationAssess2000]. Results from the validations sets (i.e., inner loops) are presented here.

The primary performance metric for model selection and evaluation of the validation set was area under the Receiver Operating Characteristic (auROC) curve [@kuhnAppliedPredictiveModeling2018]. auROC indexes the probability that the model will predict a higher score for a randomly selected positive case (lapse) relative to a randomly selected negative case (no lapse). This metric was selected because it 1) combines sensitivity and specificity, which are both important characteristics for clinical implementation; 2) is an aggregate metric across all decision thresholds, which is important because optimal decision thresholds may differ across settings and goals; and 3) is unaffected by class imbalance, which is important for comparing models with differing prediction window widths and levels of class imbalance. The best model configuration was selected using median auROC across all validation sets.

Shapley values were computed in log-odd units as our interpretability metric. Shapley values measure the unique contribution of features in an algorithm's predictions and therefore identify the relative importance of difference features [@lundbergUnifiedApproachInterpreting2017]. Global feature importance for each broad feature category was calculated by averaging the absolute values of Shapley values across all observations per feature category. Highly important features represent relevant, actionable potential antecedents to lapse (and therefore points of intervention) that will be relevant in the future development of a continuous risk monitoring system. However, these are descriptive analyses because standard errors or other indices of uncertainty for importance scores are not available for Shapley values.

Finally, a Bayesian hierarchical generalized linear model was used to estimate the posterior probability distributions and 95% Bayesian credible intervals (CIs) for auROC.

### Algorithmic fairness

Subgroups were defined on the basis of personal individual characteristics divided such that groups reflected coarse dichotomies of groups which experience relatively increased and decreased societal privilege. This resulted in four broad classes: white versus non-white, younger than 55 versus equal to or older than 55, above or below the federal poverty line [@mba2024FederalPoverty2024], and sex at birth (male versus female). A Bayesian hierarchical generalized linear model was used to estimate the posterior probability distributions and 95% Bayesian CIs for auROC across these four classes. Model contrasts were used in order to identify the likelihood of differential performance of our model between subgroups within each class.

## Results

```{r}
#| echo: false

library(tidyverse)
library(janitor)
library(kableExtra)

options(conflicts.policy = "depends.ok")
devtools::source_url("https://github.com/jjcurtin/lab_support/blob/main/format_path.R?raw=true", 
                      sha1 = "a58e57da996d1b70bb9a5b58241325d6fd78890f")

path_shared <- format_path("studydata/risk/data_processed/shared")
path_gps <- format_path("studydata/risk/data_processed/gps")
path_models <- format_path(str_c("studydata/risk/models/gps"))

pp_tidy <-  read_csv(here::here("objects", "pp_tidy.csv"),
                     show_col_types = FALSE) 

q <- c(0.025, 0.5, 0.975)
      
pp_tidy <- pp_tidy |> 
  group_by(model) |> 
  summarize(median = quantile(posterior, probs = q[2]),
            lower = quantile(posterior, probs = q[1]), 
            upper = quantile(posterior, probs = q[3]))

pp_race_con <-  read_csv(here::here("objects", "pp_race_con.csv"),
                     show_col_types = FALSE) 

pp_sex_con <-  read_csv(here::here("objects", "pp_sex_con.csv"),
                     show_col_types = FALSE) 

pp_age_con <-  read_csv(here::here("objects", "pp_age_con.csv"),
                     show_col_types = FALSE)

pp_income_con <-  read_csv(here::here("objects", "pp_income_con.csv"),
                     show_col_types = FALSE)

probs <- read_csv(here::here(path_models, str_c("best_config_v6_nested_1_x_10_3_x_10_main.csv")))
```

```{r}
#| message: false
#| warning: false

study_dates <- read_csv(here::here(path_gps, "study_dates.csv"),
                          show_col_types = FALSE) 

subids_dates <- study_dates |>
    pull(subid) |>  
    unique()

screen <- read_csv(file.path(path_shared, "screen.csv"), 
                   col_types = cols()) |>
  filter(subid %in% subids_dates) |> 
  mutate(across(dsm5_1:dsm5_11, ~ recode(., "No" = 0, "Yes" = 1))) |>  
  rowwise() |>  
  mutate(dsm5_total = sum(c(dsm5_1, dsm5_2, dsm5_3, dsm5_4, dsm5_5, dsm5_6, dsm5_7, 
                              dsm5_8, dsm5_9, dsm5_10, dsm5_11))) |>  
  ungroup()

lapses <- read_csv(file.path(path_shared, "lapses_day.csv"), col_types = cols()) |>
  filter(exclude == FALSE)

n_total <- 146

dem <- screen |>
  summarise(mean = as.character(round(mean(dem_1, na.rm = TRUE), 1)),
            SD = as.character(round(sd(dem_1, na.rm = TRUE), 1)),
            min = as.character(min(dem_1, na.rm = TRUE)),
            max = as.character(max(dem_1, na.rm = TRUE))) |>
  mutate(var = "Age",
         n = as.numeric(""),
         perc = as.numeric("")) |>
  select(var, n, perc, everything()) |>
  full_join(screen |>
  select(var = dem_2) |>
  group_by(var) |>
  summarise(n = n()) |>
  mutate(perc = (n / sum(n)) * 100), by = c("var", "n", "perc")) |>
  full_join(screen |>
  select(var = dem_3) |>
  mutate(var = fct_relevel(factor(var,
                         c("American Indian/Alaska Native", "Asian", "Black/African American",
                           "White/Caucasian", "Other/Multiracial")))) |>
  group_by(var) |>
  summarise(n = n()) |>
  mutate(perc = (n / sum(n)) * 100), by = c("var", "n", "perc")) |>
  full_join(screen |>
  select(var = dem_4) |>
  mutate(var = case_when(var == "No, I am not of Hispanic, Latino, or Spanish origin" ~ "No",
                         TRUE ~ "Yes"),
         var = fct_relevel(factor(var, c("Yes", "No")))) |>
  group_by(var) |>
  summarise(n = n()) |>
  mutate(perc = (n / sum(n)) * 100), by = c("var", "n", "perc")) |>
  full_join(screen |>
  select(var = dem_5) |>
  mutate(var = fct_relevel(factor(var,
                         c("Less than high school or GED degree", "High school or GED",
                           "Some college", "2-Year degree", "College degree", "Advanced degree")))) |>
  group_by(var) |>
  summarise(n = n()) |>
  mutate(perc = (n / sum(n)) * 100), by = c("var", "n", "perc")) |>
  full_join(screen |>
  select(var = dem_6, dem_6_1) |>
  mutate(var = case_when(dem_6_1 == "Full-time" ~ "Employed full-time",
                         dem_6_1 == "Part-time" ~ "Employed part-time",
                         TRUE ~ var)) |>
  mutate(var = fct_relevel(factor(var,
                         c("Employed full-time", "Employed part-time", "Full-time student",
                           "Homemaker", "Disabled", "Retired", "Unemployed",
                           "Temporarily laid off, sick leave, or maternity leave",
                           "Other, not otherwise specified")))) |>
  group_by(var) |>
  summarise(n = n()) |>
  mutate(perc = (n / sum(n)) * 100), by = c("var", "n", "perc")) |>
  full_join(screen |>
  summarise(mean = format(round(mean(dem_7, na.rm = TRUE), 0), big.mark = ","),
            SD = format(round(sd(dem_7, na.rm = TRUE), 0), big.mark = ","),
            min =format(round(min(dem_7, na.rm = TRUE), 0), big.mark = ","),
            max = format(round(max(dem_7, na.rm = TRUE), 0), scientific = FALSE, big.mark = ",")) |>
  mutate(var = "Personal Income",
        n = as.numeric(""),
        perc = as.numeric(""),
        mean = str_c("$", as.character(mean)),
        SD = str_c("$", as.character(SD)),
        min = str_c("$", as.character(min)),
        max = as.character(max)) |>
  select(var, n, perc, everything()), by = c("var", "n", "perc", "mean", "SD", "min", "max")) |>
  full_join(screen |>
  select(var = dem_8) |>
  mutate(var = case_when(var == "Never Married" ~ "Never married",
                         TRUE ~ var)) |>
  mutate(var = fct_relevel(factor(var,
                         c("Never married", "Married", "Divorced", "Separated",
                           "Widowed")))) |>
  group_by(var) |>
  summarise(n = n()) |>
  mutate(perc = (n / sum(n)) * 100), by = c("var", "n", "perc"))

auh <- screen |>
  summarise(mean = mean(auh_1, na.rm = TRUE),
            SD = sd(auh_1, na.rm = TRUE),
            min = min(auh_1, na.rm = TRUE),
            max = max(auh_1, na.rm = TRUE)) |>
  mutate(var = "Age of first drink",
        n = as.numeric(""),
        perc = as.numeric("")) |>
  select(var, n, perc, everything()) |>
  full_join(screen |>
  summarise(mean = mean(auh_2, na.rm = TRUE),
            SD = sd(auh_2, na.rm = TRUE),
            min = min(auh_2, na.rm = TRUE),
            max = max(auh_2, na.rm = TRUE)) |>
  mutate(var = "Age of regular drinking",
        n = as.numeric(""),
        perc = as.numeric("")) |>
  select(var, n, perc, everything()), by = c("var", "n", "perc", "mean", "SD",
                                             "min", "max")) |>
  full_join(screen |>
  summarise(mean = mean(auh_3, na.rm = TRUE),
            SD = sd(auh_3, na.rm = TRUE),
            min = min(auh_3, na.rm = TRUE),
            max = max(auh_3, na.rm = TRUE)) |>
  mutate(var = "Age at which drinking became problematic",
        n = as.numeric(""),
        perc = as.numeric("")) |>
  select(var, n, perc, everything()), by = c("var", "n", "perc", "mean", "SD",
                                             "min", "max")) |>
  full_join(screen |>
  summarise(mean = mean(auh_4, na.rm = TRUE),
            SD = sd(auh_4, na.rm = TRUE),
            min = min(auh_4, na.rm = TRUE),
            max = max(auh_4, na.rm = TRUE)) |>
  mutate(var = "Age of first quit attempt",
        n = as.numeric(""),
        perc = as.numeric("")) |>
  select(var, n, perc, everything()), by = c("var", "n", "perc", "mean", "SD",
                                             "min", "max")) |>
  full_join(screen |>
  # filter out 2 people with 100 and 365 reported quit attempts - will make footnote in table
  filter(auh_5 < 100) |>
  summarise(mean = mean(auh_5, na.rm = TRUE),
            SD = sd(auh_5, na.rm = TRUE),
            min = min(auh_5, na.rm = TRUE),
            max = max(auh_5, na.rm = TRUE)) |>
  mutate(var = "Number of Quit Attempts*",
        n = as.numeric(""),
        perc = as.numeric("")) |>
  select(var, n, perc, everything()), by = c("var", "n", "perc", "mean", "SD",
                                             "min", "max")) |>
  full_join(screen |>
  select(var = auh_6_1) |>
  mutate(var = case_when(var == "Long-Term Residential Treatment (more than 6 months)" ~ "Long-term residential (6+ months)",
                         TRUE ~ var)) |>
  group_by(var) |>
  drop_na() |>
  summarise(n = n()) |>
  mutate(perc = (n / n_total) * 100), by = c("var", "n", "perc")) |>
  full_join(screen |>
  select(var = auh_6_2) |>
  mutate(var = case_when(var == "Short-Term Residential Treatment (less than 6 months)" ~ "Short-term residential (< 6 months)",
                         TRUE ~ var)) |>
  group_by(var) |>
  drop_na() |>
  summarise(n = n()) |>
  mutate(perc = (n / n_total) * 100), by = c("var", "n", "perc")) |>
  full_join(screen |>
  select(var = auh_6_3) |>
  mutate(var = case_when(var == "Outpatient Treatment" ~ "Outpatient",
                         TRUE ~ var)) |>
  group_by(var) |>
  drop_na() |>
  summarise(n = n()) |>
  mutate(perc = (n / n_total) * 100), by = c("var", "n", "perc")) |>
  full_join(screen |>
  select(var = auh_6_4) |>
  mutate(var = case_when(var == "Individual Counseling" ~ "Individual counseling",
                         TRUE ~ var)) |>
  group_by(var) |>
  drop_na() |>
  summarise(n = n()) |>
  mutate(perc = (n / n_total) * 100), by = c("var", "n", "perc")) |>
  full_join(screen |>
  select(var = auh_6_5) |>
  mutate(var = case_when(var == "Group Counseling" ~ "Group counseling",
                         TRUE ~ var)) |>
  group_by(var) |>
  drop_na() |>
  summarise(n = n()) |>
  mutate(perc = (n / n_total) * 100), by = c("var", "n", "perc")) |>
  full_join(screen |>
  select(var = auh_6_6) |>
  group_by(var) |>
  drop_na() |>
  summarise(n = n()) |>
  mutate(perc = (n / n_total) * 100), by = c("var", "n", "perc")) |>
  full_join(screen |>
  select(var = auh_6_7) |>
  group_by(var) |>
  drop_na() |>
  summarise(n = n()) |>
  mutate(perc = (n / n_total) * 100), by = c("var", "n", "perc")) |>
  full_join(screen |>
  select(var = auh_7) |>
  mutate(var = fct_relevel(factor(var, c("Yes", "No")))) |>
  group_by(var) |>
  summarise(n = n()) |>
  mutate(perc = (n / sum(n)) * 100), by = c("var", "n", "perc")) |>
  full_join(screen |>
  summarise(mean = mean(dsm5_total),
            SD = sd(dsm5_total),
            min = min(dsm5_total, na.rm = TRUE),
            max = max(dsm5_total, na.rm = TRUE)) |>
  mutate(var = "DSM-5 Alcohol Use Disorder Symptom Count",
        n = as.numeric(""),
        perc = as.numeric("")) |>
  select(var, n, perc, everything()), by = c("var", "n", "perc", "mean", "SD",
                                             "min", "max")) |>
  full_join(screen |>
  select(var = assist_2_1) |>
  filter(var != "Never" & !is.na(var)) |>
  mutate(var = "Tobacco products (cigarettes, chewing tobacco, cigars, etc.)") |>
  group_by(var) |>
  drop_na() |>
  summarise(n = n()) |>
  mutate(perc = (n / n_total) * 100), by = c("var", "n", "perc")) |>
  full_join(screen |>
  select(var = assist_2_2) |>
  filter(var != "Never" & !is.na(var)) |>
  mutate(var = "Cannabis (marijuana, pot, grass, hash, etc.)") |>
  group_by(var) |>
  drop_na() |>
  summarise(n = n()) |>
  mutate(perc = (n / n_total) * 100), by = c("var", "n", "perc")) |>
  full_join(screen |>
  select(var = assist_2_3) |>
  filter(var != "Never" & !is.na(var)) |>
  mutate(var = "Cocaine (coke, crack, etc.)") |>
  group_by(var) |>
  drop_na() |>
  summarise(n = n()) |>
  mutate(perc = (n / n_total) * 100), by = c("var", "n", "perc")) |>
  full_join(screen |>
  select(var = assist_2_4) |>
  filter(var != "Never" & !is.na(var)) |>
  mutate(var = "Amphetamine type stimulants (speed, diet pills, ecstasy, etc.)") |>
  group_by(var) |>
  drop_na() |>
  summarise(n = n()) |>
  mutate(perc = (n / n_total) * 100), by = c("var", "n", "perc")) |>
  full_join(screen |>
  select(var = assist_2_5) |>
  filter(var != "Never" & !is.na(var)) |>
  mutate(var = "Inhalants (nitrous, glue, petrol, paint thinner, etc.)") |>
  group_by(var) |>
  drop_na() |>
  summarise(n = n()) |>
  mutate(perc = (n / n_total) * 100), by = c("var", "n", "perc")) |>
  full_join(screen |>
  select(var = assist_2_6) |>
  filter(var != "Never" & !is.na(var)) |>
  mutate(var = "Sedatives or sleeping pills (Valium, Serepax, Rohypnol, etc.)") |>
  group_by(var) |>
  drop_na() |>
  summarise(n = n()) |>
  mutate(perc = (n / n_total) * 100), by = c("var", "n", "perc")) |>
  full_join(screen |>
  select(var = assist_2_7) |>
  filter(var != "Never" & !is.na(var)) |>
  mutate(var = "Hallucinogens (LSD, acid, mushrooms, PCP, Special K, etc.)") |>
  group_by(var) |>
  drop_na() |>
  summarise(n = n()) |>
  mutate(perc = (n / n_total) * 100), by = c("var", "n", "perc")) |>
  full_join(screen |>
  select(var = assist_2_8) |>
  filter(var != "Never" & !is.na(var)) |>
  mutate(var = "Opioids (heroin, morphine, methadone, codeine, etc.)") |>
  group_by(var) |>
  drop_na() |>
  summarise(n = n()) |>
  mutate(perc = (n / n_total) * 100), by = c("var", "n", "perc"))

lapses_per_subid <- screen |>
  select(subid) |>
  left_join(lapses |>
  tabyl(subid) |>
  select(-percent), by = "subid") |>
  mutate(n = if_else(is.na(n), 0, n),
         lapse = if_else(n > 0, "yes", "no"))

lapse_info <- lapses_per_subid |>
  group_by(lapse) |>
  rename(var = lapse) |>
  mutate(var = factor(var, levels = c("yes", "no"), labels = c("Yes", "No"))) |>
  summarise(n = n()) |>
  mutate(perc = (n / n_total) * 100,
         mean = NA_real_,
         SD = NA_real_,
         min = NA_real_,
         max = NA_real_) |>
  full_join(lapses_per_subid |>
  summarise(mean = mean(n),
            SD = sd(n),
            min = min(n),
            max = max(n)) |>
  mutate(var = "Number of reported lapses"),
  by = c("var", "mean", "SD", "min", "max"))
```

### Demographics

A total of 192 individuals were eligible to participate in the study, of which 191 consented to participate and 169 enrolled in the study. Fifteen participants were excluded prior to the first monthly follow-up visit. One participant was excluded for not maintaining a recovery goal of abstinence during their time on study. Two participants were excluded due to evidence of low compliance and careless responding. A further five individuals were excluded due to poor geolocation data quality as a result of insufficient data (resulting from software incompatibility), resulting in a final sample size of 146.

The average age of the final sample was 40.9 years (SD = 12 years, range = 21-72 years). There was an approximately equal number of men (n = 74, 50.7%) and women (n = 72, 49.3%). The majority of the sample was White/Caucasian (n = 127, 86.99%) and non-Hispanic (n = 142, n = 97%). The mean income of participants was \$34,408 (SD = \$32,259, range = \$0-\$200,000). On average, participants self-reported a mean number of 8.9 DSM-V symptoms of AUD (range = 4-11). A detailed breakdown of participant characteristics is presented in @tbl-demo-2.

```{r}
# 
# options(knitr.kable.NA = "—")
# #options(knitr.table.format = "markdown")
# 
# 
# footnote_table_dem_a <- "N = 146"
# 
# footnote_table_dem_b <- "Two participants reported 100 or more quit attempts. We removed these outliers prior"
# 
# footnote_table_dem_c <- "to calculating the mean (M), standard deviation (SD), and range."
# 
# dem  |>
#   bind_rows(auh |>
#               mutate(across(mean:max, ~round(.x, 1))) |>
#               mutate(across(mean:max, ~as.character(.x)))) |>
#   bind_rows(lapse_info |>
#               mutate(across(mean:max, ~round(.x, 1))) |>
#               mutate(across(mean:max, ~as.character(.x)))) |>
#   mutate(range = str_c(min, "-", max)) |>
#   select(-c(min, max)) |>
#   kbl(longtable = TRUE,
#       booktabs = TRUE,
#       col.names = c("", "N", "%", "M", "SD", "Range"),
#       align = c("l", "c", "c", "c", "c", "c"),
#       digits = 1,
#       caption = "Demographics and clinical characteristics") |>
#   kable_styling(position = "l") |>
#   row_spec(row = 0, align = "c", italic = TRUE) |>
#   column_spec(column = 1, width = "18em") |>
#   pack_rows("Sex", 2, 3, bold = FALSE) |>
#   pack_rows("Race", 4, 8, bold = FALSE) |>
#   pack_rows("Hispanic, Latino, or Spanish Origin", 9, 10, bold = FALSE) |>
#   pack_rows("Education", 11, 16, bold = FALSE) |>
#   pack_rows("Employment", 17, 25, bold = FALSE) |>
#   pack_rows("Marital Status", 27, 31, bold = FALSE) |>
#   pack_rows("Alcohol Use Disorder Milestones", 32, 35, bold = FALSE) |>
#   pack_rows("Lifetime History of Treatment (Can choose more than 1)", 37, 43, bold = FALSE) |>
#   pack_rows("Received Medication for Alcohol Use Disorder", 44, 45, bold = FALSE) |>
#   pack_rows("Current (Past 3 Month) Drug Use", 47, 54, bold = FALSE) |>
#   pack_rows("Reported 1 or More Lapse During Study Period", 55, 56, bold = FALSE) |>
#   kableExtra::footnote(general = c(footnote_table_dem_a, footnote_table_dem_b, footnote_table_dem_c), escape=FALSE) |> 
#   save_kable(file = "objects/table.png")
```

### Model Evaluation

We selected and evaluated the best performing XGBoost model across all validation sets. This may result in a slight optimization bias in our model performance, though we believe this is largely offset through our use of 10 x 30 cross-validation (which averages model performance across 300 folds). Cross-validation maintains separation between data used to train the models, select the best models, and evaluate those best models, thereby minimizing optimization bias [@jonathanUseCrossvalidationAssess2000]. Evaluating the validation set was important to do because model development is still in progress, and as such it would not have been appropriate to examine independent test set performance at this stage.

The median auROC over all validation sets was `r sprintf("%1.3f", probs |> pull(roc_auc) |> median())`. auROCs above .7 are typically considered as having "acceptable" performance and indicate that the model correctly assigns a higher probability of lapse to a positive case (rather than a negative case) 70% of the time [@mandrekarReceiverOperatingCharacteristic2010]. @fig-auroc-histogram displays a histogram of model performance distribution across all folds.

Posterior probability distributions for the auROCs for our best performing validation set model were then used to formally characterize model performance. The median auROC was `r sprintf("%1.3f", pp_tidy$median)` (95% CI [`r sprintf("%1.2f", pp_tidy$lower)`-`r sprintf("%1.2f", pp_tidy$upper)`]), indicating that there is a probability \> .95 that our model is performing above chance (i.e., auROC \> .5; @fig-pp).

Next, we performed model calibration in order to improve our trust in model predictions. Results of model calibration are displayed in @fig-calibration, showing that this model *over* predicts lapse probability even after calibrating the model. In other words, our model is more likely to predict that an individual will lapse than the true observed rate of lapse in our sample.

Finally, a receiver operating characteristic curve is displayed in @fig-auroc-plot, representing aggregate predicted lapse logistic (calibrated) probabilities across all validation sets.

### Feature Importance

Global importance (mean absolute Shapley values) for feature categories is shown in @fig-shaps-group. Three aggregated feature categories were identified as being particularly important in contributing to model predictions: time spent at risky locations, time spent at different types of location, and time spent at locations with varying levels of alcohol availability. Other aggregated feature groups, both context-supplemented and independent, did not appear to be strong, unique global contributors to model predictions.

### Algorithmic Fairness

@fig-fairness-subgroups shows differences in model performance across race (*N* white = 127, *N* non-white = 19), sex (*N* male = 74, *N* female = 72), age (*N* younger than 55 = 126, *N* older than or equal to 55 = 20), and income (*N* below federal poverty line = 48, *N* above federal poverty line = 98). All group comparisons were reliably different (probability \> .95) across models, such that identities with higher assumed privilege were associated with improved model performance. White, non-Hispanic participants demonstrated `r sprintf("%1.3f", pp_race_con$median)` greater model performance than Hispanic and/or non-white participants (range=`r sprintf("%1.3f", pp_race_con$lower)`-`r sprintf("%1.3f", pp_race_con$upper)`, probability=`r sprintf("%1.3f", pp_race_con$prob)`). Male participants demonstrated `r sprintf("%1.3f", pp_sex_con$median)` greater model performance than female participants (range=`r sprintf("%1.3f", pp_sex_con$lower)`-`r sprintf("%1.3f", pp_sex_con$upper)`, probability=`r sprintf("%1.3f", pp_sex_con$prob)`). Younger participants demonstrated `r sprintf("%1.3f", pp_age_con$median)` greater model performance than older participants (range=`r sprintf("%1.3f", pp_age_con$lower)`-`r sprintf("%1.3f", pp_age_con$upper)`, probability=`r sprintf("%1.3f", pp_age_con$prob)`). Finally, participants above the poverty line demonstrated `r sprintf("%1.3f", pp_income_con$median)` greater model performance than those below the poverty line (range=`r sprintf("%1.3f", pp_income_con$lower)`-`r sprintf("%1.3f", pp_income_con$upper)`, probability=`r sprintf("%1.3f", pp_income_con$prob)`).

## Discussion

### Model Performance

Our day-level model of lapse prediction using geolocation data performs at an "acceptable" threshold [auROC between .7 and .8; @mandrekarReceiverOperatingCharacteristic2010], suggesting that, while there is still a considerable amount of improvement to be made in model performance, geolocation data can predict future alcohol lapse in the next day with fair sensitivity and specificity. Bayesian model comparisons corroborated that this model performed unilaterally better than chance (auROC = .5). This study also provided explanatory insights by way of quantifying feature importance as well as a crucial examination of model fairness across subgroups, detailed in the following section.

Model calibration is the process of fine-tuning model predictions to more closely align with the true likelihood of a given outcome and was carried out in order to improve our trust in model predictions (in this case, to better align model predictions against the observed lapse rate in our sample; @dormannCalibrationProbabilityPredictions2020). XGBoost is not a probabilistic model and as such it is expected that probabilities would need to be calibrated. Yet, even following calibration, our model overpredicts occurrence of lapses in our sample. Identifying this in the validation stage enables us to make further changes to our algorithm, such as refitting the model, prior to moving onto the final evaluation stage [@vancalsterCalibrationAchillesHeel2019]. Two potential solutions would be to explore other calibration methods outside of logistic calibration, such as beta calibration, and to examine the distribution of our feature set. Typically class imbalances result in an overprediction of the majority class and not the minority class (here, we would expect an overprediction of no lapses relative to lapses). Instead, we see the opposite. This may be because our features favor the minority class. For example, time spent at risky locations, our most predictive feature, is a unipolar scale that is focused on the *riskiness* and not the *protectiveness* of a location. However, it should be noted that this oversensitivity may not be an issue depending on what information we hope to relay to individuals using a continuous risk monitoring system. In the future development of such a system, we are not necessarily interested in communicating exact probabilities to individuals about their lapse risk (such as, *"There is a 92% chance that you lapse back to use today"*). Rather, we are more interested in communicating *relative* levels of risk (such as, *"You are at a low risk level of lapse today,"* or, *"Your risk of lapse is higher this week compared to last week"* where *low risk* corresponds to a designated probability threshold).

We used Shapley values to quantify global feature importance. The top performing Shapley values were time spent at risky locations, time spent at different location types (e.g., home, bars, work), and time spent at locations with varying levels of alcohol availability. Time spent at risky locations was associated with a 2x *greater* log-odds change in lapse risk as compared to the next highest performing feature of time spent at different location types. These results are well-aligned with the extant AUD literature, notably the focus of high-risk situations as an immediate determinant to relapse within the relapse prevention model [@larimerRelapsePreventionOverview1999b; @marlattRelapsePreventionMaintenance1985]. These three features were all generated utilizing additional context supplied by participants after a given location was identified as frequently visited (\> 2x in the previous month). However, it should be noted that these features may be able to be generated without user feedback. For example, location types could be classified using public map data and consumer data could be used to identify establishments that sell alcohol. This could further reduce the burden on an individual using a continuous risk monitoring system by not requiring individual input. On the other hand, self-classifying locations as risky might be encoding nuance that could not be feasibly obtained using public data. For instance, a location might be labeled as risky from user input because it is a person-specific triggering location (e.g., scene of a traumatic event).

Interestingly, location valence, or the emotion tied to a given location, is the fourth-highest Shapley value, yet appears to be minimally contributing to model predictions. This may be because participants were asked retrospectively about these locations at one month follow-up visits, and so our measures of emotional quality of a location may be too distal to be meaningful. While other measures of global feature importance were low, it will be important to examine local feature importance to better understand their potential utility. For example, a given feature may have low global importance (low mean absolute Shapley value), but may have a unidirectional relationship with a particular class (e.g., always associated with lapse predictions).

### Model Fairness

All models exhibited differential performance across subgroups of race/ethnicity, sex at birth, age, and income, such that model performance was worse for non-white, female, and older participants, as well as those below the poverty line. One of the driving forces behind these disparities is a lack of representation in these data. For instance, even collapsing across dichotomous categories for both race and ethnicity (i.e., white and non-Hispanic and non-white and/or Hispanic), our non-white and/or Hispanic sample only reflects 13% of the total sample. Outside of recruiting a more diverse sample in future studies, one potential solution is to synthetically up-sample cases of the minority class (in this case, non-white participants) such that the model has more data on which to base its predictions [@kabirBalancingFairnessUnveiling2024].

Yet, we also see divergent performance across men and women in our sample, a class that *is* well-balanced (*N* male = 74, *N* female = 72). This may be due, as predicted, to how constructs of AUD have been conceptualized. The historical literature has been built primarily on studying the experiences of AUD in men. Our features may not be as salient of predictors for lapse for women as they are for men, resulting in the differences in auROCs that we see here. This underscores the importance of *procedural* fairness (i.e., considering the fairness of feature inputs themselves) in addition to outcome fairness [@grgic-hlacaDistributiveFairnessAlgorithmic2018]. Using this *means* and *ends* approach may result in a more refined understanding of different facets impacting performance across groups.

If maintaining a top-down approach to feature selection from the literature is a primary goal, more research will need to be done to create a better understanding of AUD experiences and recovery for specific groups who have historically been under-studied in this area. There have been several recent calls to action with respect to studying AUD across sex suggesting that factors which precipitate lapse and contribute to recovery maintenance may be different for women as compared to men [@mccaulAlcoholWomenBrief2019; @mccradyTreatmentInterventionsWomen2020]. Alternatively, bottom-up approaches, such as allowing an algorithm to identify patterns in the data that are most predictive of lapse, may get around issues of bringing in biased features from the literature. This may be at the expense of generating features that are not easily clinically translatable or particularly meaningful. Together, these results suggest that we are seeing the aftereffects of both statistical bias (i.e., inadequate sampling) and societal bias (i.e., constructs which are of limited value to certain groups) in our model [@mitchellAlgorithmicFairnessChoices2021].

While it is important to consider ways to remedy this in our model so that it may be able to be equitably used in the real-world, it is also important to note that the goal of this work is not to suggest that quantitative definitions of fairness are sufficient to fix deeply rooted issues of societal injustice [@greenEscapingImpossibilityFairness2022; @greenMythMethodologyRecontextualization; @ochigameLongHistoryAlgorithmic2020].

### Limitations and future directions

This study is an important step forward in building a model using geolocation data to predict next-day lapse back to drinking in individuals with a diagnosis of AUD and a recovery goal of abstinence. However, there are several limitations to this work.

First, our model has only a fair level of performance. A simple strategy to improve this is to explore additional features. Risk-relevant features that can be generated without additional participant-supplied contextual information include geographical spatial risk indicators and circadian rhythm data. Risk-terrain modeling is a spatial analysis technique which enables the estimation of environmental risk factors on a given outcome, such as contact with high liquor outlet density or high crime areas preceding lapse [@gajosUsingRiskTerrain]. Circadian rhythm data, on the other hand, can be extracted using a Lomb-Scargle periodogram to derive the spectrum of geolocation data, therefore quantifying the (ir)regularity of daily activity [@vanderplasUnderstandingLombScargle2018]. Circadian rhythm patterns derived from geolocation data have been widely used in the affective science literature for predicting mood [@chikersalDetectingDepressionPredicting2021; @saebMobilePhoneSensor2015b; @saebRelationshipClinicalMomentary2015a]. Moreover, encouraging the maintenance of a daily routine may be helpful in the context of AUD recovery specifically [@huhneDAILYPersonalizedCircadian2021; @tamuraCircadianRhythmsSubstance2021]. These features are not only clinically meaningful, but are also clinically *intervenable*. In the context of a continuous risk monitoring system, individuals could receive notifications relating to increasing amounts of time spent in riskier areas (beyond self-identified risky locations) or irregularity of movement patterns suggesting circadian disruption. Potential interventions in these examples would be to suggest relocating to a "safer" area or encouraging following a more regular schedule. We might also be interested in features that, while not being clinically-actionable, may add more predictive value to our model by increasing precision. Examples of such features are day of the week and weather, two factors that contribute to daily behavior and thus geolocation data [@hellerAssociationRealworldExperiential2020].

Secondly, we assessed auROCs from our validation sets against change performance (auROC = .5). While this is an important standard to meet, it may be more useful to instead design a baseline model against which to compare performance. For example, we could compare the predictive power of our model against a model which predicts future lapse from past lapse behavior [as in @wyantMachineLearningModels2023]. Alternatively, we might be interested in comparing our model against another model using more simplistic features that we think might be related to both geolocation patterns and drinking behavior, such as a model which uses day of the week to predict lapse. A more rigorous model contrast will set a higher standard with which to evaluate our model against prior to implementation in a real-world context.

Next, we have examined the feature importance and have identified three geolocation features that appear to be strongly predictive of next-day lapse: time spent at risky locations, time spent across various types of places, and time spent at locations where alcohol is available. These features are collapsed across particular response-types and across time periods. A first next step will be to break down these top performing features into their subcomponents, such as looking at the contributions of time spent at high, medium, and low risk locations to lapse predictions separately. It may also be of value to look at particular time points. Duration features in this model were calculated at intervals starting from 6 hours prior up until 168 hours prior. In the context of geolocation data, it may be interesting to more closely exmaine more proximal as opposed to distal time points. These two avenues are both still at the global (i.e., across all subjects) level of feature importance. We can also examine how these features perform at an individual level for a given prediction to get a sense of how stable they are over time, within and between people.

Finally, following final evaluation using an independent test set, a future direction of this work is to integrate the most-predictive features derived from this geolocation-based model with features from an ecological momentary assessment (EMA)-based model designed within our lab [@wyantMachineLearningModels2023]. Geolocation features may provide unique insight into lapse risk that survey-based assessments do not (or cannot) capture, and may also enable the use of less-frequent questionnaires. The final model built from these two separate endeavors will be used to optimize the development of a continuous risk monitoring system for AUD in a recently funded grant (1R01AA031762-01).

### Conclusion

This study demonstrates that it is feasible to predict lapse with a fair level of accuracy using geolocation data, suggesting that geolocation data is a viable supplement for risk prediction monitoring systems. However, our model demonstrates differential performance across vulnerable subgroups. Moving forward, additional risk-relevant features will be added to the model in an effort to improve prediction and the final model will be evaluated.

## References

::: {#refs}
:::

\newpage

## Tables and Figures

<!--************************************************************************-->
<!-- Table 1: Eligibility Criteria-->
### Table 1: Eligibility Criteria

| Eligibility Criteria                                           |
|----------------------------------------------------------------|
| \>= 18 years of age                                            |
| Ability to read and write in English                           |
| Diagnosis of moderate AUD (\>= 4 self-reported DSM-5 symptoms) |
| Abstinent from alcohol for 1-8 weeks                           |
| Willing to use only one smartphone\*\* while on study          |

: Eligibility criteria for study enrollment. \*\*Personal or study-provided. {#tbl-elig}

```{=html}
<br>
<br>
<br>
<br>
<br>
```

\newpage

<!--************************************************************************-->
<!-- Table 2: Collected demographic information-->
### Table 2: Collected demographic information

| Variable     | Measure                                                           |
|--------------------------|----------------------------------------------|
| Demographics | Age                                                               |
|              | Sex                                                               |
|              | Race                                                              |
|              | Ethnicity                                                         |
|              | Employment                                                        |
|              | Income                                                            |
|              | Marital Status                                                    |
| Alcohol      | Alcohol Use History                                               |
|              | DSM-5 Checklist for AUD                                           |
|              | Young Adult Alcohol Problems Test                                 |
|              | WHO-The Alcohol, Smoking and Substance Involvement Screening Test |

: Demographic and relevant alcohol use history variables sampled at screening visit. {#tbl-demo-1}

```{=html}
<br>
<br>
<br>
<br>
<br>
```

\newpage

<!--************************************************************************-->
<!-- Table 3: Contextual geolocation information-->
### Table 3: Contextual geolocation information

| Question                                                                                                      | Responses                                                                                                                                                                                                                                                                                      |
|-----------------------|-------------------------------------------------|
| Address                                                                                                       |                                                                                                                                                                                                                                                                                                |
| Type of place                                                                                                 | Work, School, Volunteer, Health care, Home of a friend, Home of a family member, Liquor store, Errands (e.g., grocery store, post office), Coffee shop or cafe, Restaurant, Park, Bar, Gym or fitness center, AA or recovery meeting, Religious location (e.g., church, mosque, temple), Other |
| Have you drank alcohol here before?                                                                           | No, Yes                                                                                                                                                                                                                                                                                        |
| Is alcohol available here?                                                                                    | No, Yes                                                                                                                                                                                                                                                                                        |
| How would you describe your experiences here?                                                                 | Pleasant, Unpleasant, Mixed, Neutral                                                                                                                                                                                                                                                           |
| Does being at this location put you at any risk to begin drinking?                                            | No risk, Low risk, Medium risk, High risk                                                                                                                                                                                                                                                      |
| Did the participant identify this place as a risky location they are trying to avoid now that they are sober? | No, Yes                                                                                                                                                                                                                                                                                        |

: Location information collected from frequently visited locations. {#tbl-context}

```{=html}
<br>
<br>
<br>
<br>
<br>
```

\newpage

<!--************************************************************************-->
<!-- Table 4: Demographic responses-->
### Table 4: Demographic responses

![Demographic characteristics of the sample (N = 146).](objects/table.png){#tbl-demo-2}

```{=html}
<br>
<br>
<br>
<br>
<br>
```

\newpage

<!--************************************************************************-->
<!-- Figure 1: auROC histogram-->
### Figure 1: auROC histogram

{{< embed notebooks/auROC_distribution_posterior.qmd#fig-auroc-histogram >}}

```{=html}
<br>
<br>
<br>
<br>
<br>
```

\newpage

<!--************************************************************************-->
<!-- Figure 2: auROC posterior probability distribution-->
### Figure 2: auROC posterior probability distribution

{{< embed notebooks/auROC_distribution_posterior.qmd#fig-pp >}}

```{=html}
<br>
<br>
<br>
<br>
<br>
```

\newpage

<!--************************************************************************-->
<!-- Figure 3: Calibration plot-->
### Figure 3: Calibration plot

{{< embed notebooks/calibration_plot.qmd#fig-calibration >}}

```{=html}
<br>
<br>
<br>
<br>
<br>
```

\newpage

<!--************************************************************************-->
<!-- Figure 4: Calibrated auROC plot-->
### Figure 4: Calibrated auROC plot

{{< embed notebooks/auROC_plot.qmd#fig-auroc-plot >}}

```{=html}
<br>
<br>
<br>
<br>
<br>
```

\newpage

<!--************************************************************************-->
<!-- Figure 5: Global Feature Importance-->
### Figure 5: Global Feature Importance

{{< embed notebooks/shaps.qmd#fig-shaps-group >}}

```{=html}
<br>
<br>
<br>
<br>
<br>
```

\newpage

<!--************************************************************************-->
<!-- Figure 6: Posterior probability distribution 95% credible intervals by subgroup-->
### Figure 6: Posterior probability distribution 95% credible intervals by subgroup

{{< embed notebooks/fairness.qmd#fig-fairness-subgroups >}}
