---
title: The Feasibility and Equity of Geolocation Data for Lapse Prediction in AUD
author:
  - name: Claire Punturieri 
    orcid: 0009-0003-7743-3736
    corresponding: false
    roles: []
    affiliations:
      - Department of Psychology, University of Wisconsin-Madison
  - name: John J. Curtin 
    orcid: 0000-0002-3286-938X
    corresponding: true
    email: jjcurtin@wisc.edu
    roles:
      - Investigation
      - Project administration
      - Software
      - Visualization
    affiliations:
      - Department of Psychology, University of Wisconsin-Madison 
keywords:
  - Substance use disorders
  - Precision mental health 
abstract: |
  Claire's FYP!
plain-language-summary: |
  Yay geolocation data!
key-points:
  - Take away point 1 
  - Take away point 2
date: last-modified
bibliography: references.bib
citation:
  container-title: Journal of Geolocation is Awesome 
number-sections: false 
editor_options: 
  chunk_output_type: console
---

## Specific Aims

Smart digital therapeutics ("smart DTx") refer to treatments delivered via technology such as smartphones that use machine learning algorithms to recommend engagement with particular in-app modules at the optimal time. In the context of alcohol use disorder (AUD), these algorithms could be used 1) to predict an oncoming lapse; and 2) to encourage use of specific tools to minimize risk of that lapse occurring based on factors specifically relevant to that individual (e.g., changes in mood, difficulty with close social connections, proximity to triggering locations). In other words, these algorithms have the potential to predict both *when* and *why* a lapse may occur. This is beneficial with respect to AUD where it may be difficult for an individual to know the exact precipitants to a lapse, even if the lapse itself is anticipated. These algorithms not only need to be developed outright, but also need to perform well (i.e., pinpoint lapses accurately) and identify meaningful, actionable lapse precursors. However, before accurate personalized recommendations can be offered, smart DTx must demonstrate similar performance across sub-groups and provide a sustainable method of data collection for users (i.e., minimally burdensome).

A fair algorithm is one with no preference in performance with respect to inherent or acquired characteristics (e.g., gender, race, socioeconomic status) [@wangBriefReviewAlgorithmic2022]. In the context of AUD, this would mean that lapse predictions are reasonably accurate and do not favor or disadvantage any particular group. The performance of an algorithm across sub-groups can be assessed using measures of *algorithmic fairness*, which quantify relative differences in predictions like accuracy. Incorrect predictions of lapse (or lack thereof) could result in both poorer outcomes and eroded trust of these tools. Assessing algorithmic fairness during the development of smart DTx seeks to ensure that the same standard of care is provided to everyone utilizing it.

The inputs to these algorithms are also important to consider. For example, many DTx use self-report measures administered several times per day (ecological momentary assessment or "EMA"). However, EMA may be burdensome for individuals who do not have the flexibility of filling out repeated assessments each day. One solution is to utilize data collected via unobtrusive means, like geolocation data, which captures facets of activity patterns such as movement, frequency, duration, and time. Smartphones come pre-equipped with sensors to be able to capture this granular data continuously. Furthermore, geolocation requires little maintenance following initial set-up. Minimal additional contextual data can also be solicited from individuals about frequently visited locations to distill targets for intervention. A smart DTx could take these salient features and identify helpful in-app modules accordingly. Taken together, supplemented geolocation data can provide insight into how activity patterns and location semantics interface to increase (or decrease) the probability of lapse.

For my first-year project, I intend to use geolocation data collected from smartphones and corresponding self-reported contextual information for frequently visited locations to build a machine learning algorithm to predict next-day alcohol use lapse among participants with a diagnosis of AUD and a recovery goal of abstinence. I will also evaluate the algorithmic fairness of the model using both algorithmic fairness metrics for discrete subgroups and statistical modeling that accounts for cross-group membership (i.e., a more intersectional approach).

To accomplish these goals, I have identified the following specific aims:

**Aim 1: Train several machine learning algorithms to predict alcohol lapse from geolocation data.** Features will be derived from continuous geolocation data in combination with contextual information about frequently visited (\>2 per one month period) locations. Several candidate classification algorithms will be trained and evaluated to predict next-day lapse.

**Aim 2: Evaluate the best performing algorithm.** Area under the Receiver Operating Characteristic Curve (auROC) will be used as the primary performance metric to select and evaluate algorithm configurations. Secondary performance metrics (e.g., sensitivity, specificity) and interpretability metrics (i.e., SHAP values) will also be used to evaluate algorithms.

**Aim 3: Evaluate the algorithmic fairness of the top performing algorithm across subgroups who experience known treatment disparities in the context of AUD.** Algorithmic fairness will be examined across --race/ethnicity, marital status, education, and sex--. Fairness will be evaluated using metrics calculated across discrete subgroups.

## Significance


Chronic, recurrent patterns of relapse back to harmful alcohol use are, for many individuals with AUD, a common occurrence during the recovery process [@mckayTreatingAlcoholismChronic2011a; @moosRatesPredictorsRelapse2006]. Lapses, or single instances of goal-inconsistent use that may lead to relapse, are therefore an important intervention target [@witkiewitzRelapsePreventionAlcohol2004a]. However, the factors that may precipitate a lapse change both person-by-person and moment-by-moment, highlighting the potential utility of smart DTx in this space and, at a necessarily earlier step, the need for algorithm development to accurately predict these outcomes. Understanding not only when a lapse may happen, but also why, are both key components to facilitate optimally effective intervention. Because it is dynamic and continuous, measures which seek to quantify risk of lapse need to be able to keep up with appropriate levels of granularity.

<!--this paragraph has a LOT in it and may be hard to follow for others.  I am lost a bit in the dtx vs. smart dtx vs algorithm pieces.  Which are important here?  Or if all, maybe break apart into separate paragraphs-->

### The role of location in lapse

The importance of location, such as environmental cues or one's perceived riskiness of a setting, has been shown to play an important role in lapse [@janakPotentEffectEnvironmental2010; @waltonIndividualSocialEnvironmental2003;  @waltonSocialSettingsAddiction1995]. This link with lapse risk has translated into the integration of coping skills relating to substance-associated contexts in several treatment strategies like mindfulness-based relapse prevention [@lecocqConsideringDrugAssociatedContexts2020]. These findings underscore the potential wealth of information relating to lapse risk that an individual's location can provide.

Some of the more nuanced facets captured within location are associations with others (or lack thereof, e.g., social isolation), associations with previous drinking behaviors (e.g., whether or not alcohol is present), and associations with affect (i.e., negative versus positive emotions tied to a given location). Moreover, these different factors may play differential roles in lapse risk across individuals. For example, for one person, spending time in locations associated with negative affect may be a strong trigger, whereas for another it may be a lack of contact with others. A key benefit to using location data to predict lapse is its ability to cast a wide net across potential triggers and, therefore, to be applicable to a wide range of people.
<!--not sure about the highlighting of different patterns for different folks.   How will your model accomodate that?-->

### Geolocation data for lapse prediction

Geolocation data, also referred to as global positioning system (GPS) data, are data which quantify spatial positioning and movement across time. Many forms of technology that are now ubiquitous in daily life, such as smartphones and smartwatches, automatically and continually collect these data. This fact, paired with increasing rates of smartphone ownership, suggest that there is high potential for these data to be harnessed for mental health applications [@areanMobileTechnologyMental2016]. In the context of substance use, geolocation data has been specifically identified as being of particular use in both understanding the precipitants to harmful use and its effective treatment [@stahlerGeospatialTechnologyExposome2013].

While previous mental health research utilizing geolocation data has shown promise, the features used and outcomes explored have largely been siloed across psychopathology subfields. For example, within the substance use literature, geolocation data have historically been used to examine risky locations, such as the influence of neighborhood characteristics on use [@epsteinRealtimeTrackingNeighborhood2014a; @kwanUncertaintiesGeographicContext2019] and individual physical proximity to locations of potential or past harmful use such as bars (either estimated using geofencing or user-defined) [@attwoodUsingMobileHealth2017; @carreiroRealizeAnalyzeEngage2021; @gonzalezComparisonSmartphoneApp2015; @gustafsonSmartphoneApplicationSupport2014a; @naughtonContextSensingMobilePhone2016]. Several of the applications implemented in these studies enable real-time notifications about locations to their users (e.g., "You are entering a high-risk zone").

On the other hand, contrary to the substance use literature, affective scientists have focused more on factors relating to mood and behavior. Geolocation data have been used to estimate loneliness and isolation [@doryabIdentifyingBehavioralPhenotypes2019a], to demonstrate increases in positive affect from seeking out novel environments [@@hellerAssociationRealworldExperiential2020], and to quantify depressive symptoms [@raughGeolocationDigitalPhenotyping2020]. Moreover, these data have not only been harnessed to measure mood symptoms but to also predict their emergence [for review, see @shinSystematicReviewLocation2023].

Latent characteristics of location -- such as isolation, mobility, novelty seeking, and the emotional valence -- are notably lacking in the substance use geolocation literature, but have historically been captured in other ways such as by way of ecological momentary assessment (EMA) data. In substance use research, EMA surveys often ask information about mood, social experiences, and daily activities. However, geolocation data provides greater temporal sensitivity and specificity compared to its EMA counterpart, which is collected at static intervals several times per day as opposed to the continuous sampling of geolocation data. While there is no doubt in the predictive utility of EMA measures in the context of lapse [@wyantMachineLearningModels2023a], the affective science literature suggests that it may be possible to quantify some of these features utilizing location data. In the context of a smart DTx, this would mean reduced patient burden (i.e., fewer surveys to fill out) and a potentially more equitable system (i.e., easier for people who can't fill out multiple surveys a day to engage with).

My first-year project will address a crucial gap in the literature between clinical science subdisciplines by applying feature engineering techniques both from the substance use and affective science literatures, therefore providing a novel, dynamic lens to the use of geolocation data in predicting lapse. Moreover, it will implement a lower burden, finer-grained temporal perspective compared to other techniques like EMA. Taking both of these limitations of previous work into account, my first-year project will develop a sensing system that can predict lapse day-to-day utilizing a broad range of low-burden features which capture facets of both movement and mood.

### Algorithmic fairness

While a great strength of machine learning algorithms is their ability to elucidate latent statistical patterns of data, its greatest weakness lies in the necessary reliance on these data. In the context of health-related data, historical patterns of health care inequities will almost certainly and unavoidably be embedded within data used to train algorithms. These <!--is that a problem for your data and are their other sources of inequity-->inequities may unintentionally be carried forward in perpetuity by machine learning models if not critically examined. Without examining algorithmic fairness prior to deployment in the real-world, smart DTx run the risk of providing sub-optimal mental health care to individuals who already face disadvantages.

Conversations around machine learning and fairness in healthcare have peppered the literature for several years [@rajkomarEnsuringFairnessMachine2018a; @wawiragichoyaEquityEssenceCall2021], yet no concrete standards of reporting of algorithmic fairness have been delineated. This is of critical importance in considering that the ultimate goal of many of these algorithms is to be implemented in patient contexts. One potential legal marker of algorithmic fairness may be derived from the Equal Employment Opportunity Commission (EEOC) 80% rule, which states that hiring rates of individuals from protected groups (e.g., people of color, women) should be at least 80% that of white men. Applied in the context of algorithmic fairness, this would be something like model performance being at least 80% of the best-performing group <!--interesting idea!-->

Despite the fact that standards of reporting do not exist, there are many established metrics for statistically assessing algorithmic fairness. Common metrics of algorithmic fairness are predicated on the assignment of a "privileged" and "non-privileged" group. Once this has been established, measures such as accuracy can be examined between those two groups [for example metrics, see: @wisniewskiFairmodelsFlexibleTool2022]. This can be repeated over many combinations of privileged and non-privileged groups to assess model performance across various identities.

At broad level, algorithmic fairness is paramount to consider in context of AUD because risk, access to and engagement with treatment, and negative consequences from drinking vary across socioeconomic status, race, and gender in AUD [@callingSocioeconomicStatusAlcohol2019; @vaethDrinkingAlcoholUse2017a; @witbrodtRacialEthnicDisparities2014]. Location plays an important role in contributing to mental health inequities in general [@valleeRoleDailyMobility2011], and in the context of substance use, exposure to substances, neighborhood disadvantage, and barriers to treatment stratified by location all contribute to the initiation and maintenance of SUDs [@mennisRiskySubstanceUse2016]. Furthermore, these inequities are not shared equally across the population: for example, alcohol outlet density is greater in predominantly Black census tracts [@scottStructuralRacismBuilt2020].
        
My first-year project will address several gaps in the mental health and algorithmic fairness literature. Firstly, it will advocate for the use of fairness metrics alongside other measures of algorithm performance. Secondly, it will apply measures to data which are known to contain these disparities in the context of substance use, geolocation data, in an effort to critically examine my best performing model.

### Current study

My proposed first-year project will apply machine learning to predict lapse in AUD utilizing geolocation data. The first goal (**Aims 1 and 2**) is to build and evaluate a model to predict oncoming lapse with an almost entirely passive data stream for individuals with a recovery goal of abstinence. The second goal (**Aim 3**) is to critically examine the fairness of the best performing model.

<!--any explanatory benefits?-->

## Approach

### Overview

For my first year project, I will conduct analyses using a subset of data collected between 2017 and 2019 under a larger grant funded though the National Institute of Alcohol Abuse and Alcoholism (R01 AA024391). As such, the following approach section represents aspects relevant only to my first year project, though other data were collected under this grant.

### Participants

One hundred and fifty one individuals in early-recovery (1-8 weeks of abstinence) for AUD were recruited from the Madison area to take part in a three-month study on how mobile health technology can provide recovery support. Recruitment approaches included social media platforms (e.g., Facebook), television and radio advertisements, and clinic referrals. Prospective participants completed a phone screen to assess match with eligibility criteria (Table 1). Participants were excluded if they exhibited severe symptoms of paranoia or psychosis (a score \<= 2.24 on the SCL-90 psychosis scale or a score \<= 2.82 on the SCL-90 paranoia scale administered at screening).

| Eligibility Criteria                                           |
|----------------------------------------------------------------|
| \>= 18 years of age                                            |
| Ability to read and write in English                           |
| Diagnosis of moderate AUD (\>= 4 self-reported DSM-5 symptoms) |
| Abstinent from alcohol for 1-8 weeks                           |
| Willing to use only one smartphone\*\* while on study          |

: Eligibility criteria for study enrollment. \*\*Personal or study-provided.

### Procedure

Participants enrolled in a three-month study consisting of five in-person visits, daily surveys, and continuous passive monitoring of geolocation data. Following screening and enrollment visits in which participants consented to participate, learned how to manage location sharing (i.e., turn off location sharing when desired), and reported frequently visited locations, participants completed three follow-up visits one month apart. At each visit, participants were asked questions about frequently visited (\>2 times during the course of the previous month) locations. Participants were debriefed at the third and final follow-up visit. Participants were expected to provide continuous geolocation data while on study. Other personal sensing data streams (EMA, cellular communications, sleep quality, and audio check-ins) were collected as part of the parent grant’s aims (R01 AA024391).

### Measures

#### Geolocation data

To enable collection of geolocation data, participants downloaded either the Moves app or the FollowMee app during the intake visit. Moves was bought-out and subsequently deprecated while the study was ongoing (July 2018) and data collection continued using FollowMee until the end of the study. Both apps continuously tracked location via GPS and WiFi positioning technology.

#### Contextual information

Contextual information for frequently visited locations (\>2 times in the previous month) was obtained during an interview at each follow-up visit (at month 1, 2, and 3; Table 2).

| Question                                                                                                      | Responses                                                                                                                                                                                                                                                                                      |
|-----------------------|-------------------------------------------------|
| Address                                                                                                       |                                                                                                                                                                                                                                                                                                |
| Type of place                                                                                                 | Work, School, Volunteer, Health care, Home of a friend, Home of a family member, Liquor store, Errands (e.g., grocery store, post office), Coffee shop or cafe, Restaurant, Park, Bar, Gym or fitness center, AA or recovery meeting, Religious location (e.g., church, mosque, temple), Other |
| Have you drank alcohol here before?                                                                           | No, Yes                                                                                                                                                                                                                                                                                        |
| Is alcohol available here?                                                                                    | No, Yes                                                                                                                                                                                                                                                                                        |
| How would you describe your experiences here?                                                                 | Pleasant, Unpleasant, Mixed, Neutral                                                                                                                                                                                                                                                           |
| Does being at this location put you at any risk to begin drinking?                                            | No risk, Low risk, Medium risk, High risk                                                                                                                                                                                                                                                      |
| Did the participant identify this place as a risky location they are trying to avoid now that they are sober? | No, Yes                                                                                                                                                                                                                                                                                        |

: Location information collected from frequently visited locations.

#### Participant characteristics

Participants completed a baseline measure of demographics and other constructs relevant to lapse at the screening visit, which will be used both for model building and for fairness assessments (Table 3).

| Variable     | Measure                                                           |
|--------------------------|----------------------------------------------|
| Demographics | Age                                                               |
|              | Sex                                                               |
|              | Race                                                              |
|              | Ethnicity                                                         |
|              | Employment                                                        |
|              | Income                                                            |
|              | Marital Status                                                    |
| Alcohol      | Alcohol Use History                                               |
|              | DSM-5 Checklist for AUD                                           |
|              | Young Adult Alcohol Problems Test                                 |
|              | WHO-The Alcohol, Smoking and Substance Involvement Screening Test |

: Demographic and relevant alcohol use history variables sampled at screening visit.

#### Definition of subgroups for fairness assessments

Subgroups will be defined on the basis of personal individual characteristics (in the machine learning fairness literature, "sensitive attributes") that are specifically associated with treatment disparities in AUD. Characteristics will be assessed using this method to ensure the development of a model that does not further exacerbate/magnify existing treatment disparities.

#### Lapses

Alcohol lapses will be used as our outcome variable in this study and will be used to provide labels for model training, for testing model performance, and for testing issues of algorithmic fairness across our predefined subgroups. Future lapse occurrence (here conceptualized as next-day lapse) will be predicted in 24-hour rolling windows, beginning at midnight on a participant's second day of participation to ensure one full day of data collection for the first window, and at every subsequent hour (i.e., future lapse in the next 24-hours predicted each hour). This rolling window approach will take advantage of the granular, continuous nature of geolocation data, enabling frequent updating of predictions. *Lapse* and *no lapse* occurrences will be identified from the daily survey question, "Have you drank any alcohol that you have not yet reported?". Participants who responded *yes* to this question were then asked to report the date and hour of the start and the end of the drinking episode. In this case, the prediction window will be labeled *lapse*. Prediction windows will be labeled *no lapse* if no alcohol use was reported within that window. Windows will be excluded if no alcohol use occurred within the window but *did* occur within six hours of the start or end of the window for label fidelity.

### Feature engineering

Feature engineering is the process of creating variables (or "features") from unprocessed data and will be used in this project to transform raw data from three sources: 1) demographic characteristics relevant to treatment disparities in AUD collected at intake; 2) day at prediction window onset; and 3) geolocation data collected the prior day. Features will be created using all GPS features combined with demographic and day/time features.

We will generate a quantitative feature for age and dummy-coded features for race/ethnicity, marital status, education, and sex. Dummy-coded features indicating time of day (5pm - midnight versus any other time) and day of the week will also be generated. As a comparison to the geolocation model, a baseline model will also be developed utilizing only the time-of-day dummy-coded features to predict lapse. Features from geolocation data will be generated that both utilize contextual information collected from monthly surveys (i.e., location valence and perceived riskiness) as well as features that are independent of further individual input.

Features will be derived using examples from previous literature and will also be generated using contextual features. For example, geo-fencing techniques to identify bars and liquor stores [@gonzalezComparisonSmartphoneApp2015; @gustafsonSmartphoneApplicationSupport2014a] have been used in the AUD literature. More broadly, features such as isolation [@doryabIdentifyingBehavioralPhenotypes2019a; @raughGeolocationDigitalPhenotyping2020] and seeking of novelty locations [@hellerAssociationRealworldExperiential2020] have been used in the affective science literature as mood proxies. In addition to the wide range of possible features from the literature, this data set affords the ability to create unique features using person-specific contextual information in tandem with geolocation data, such as places where an individual has drank in the past and the emotional valence associated with that location. We will also calculate raw and change features based on previous geolocation data in order to capture individual variation.

Imputation of missing data and removal of zero-variance features are additional general processing steps that will also be undertaken during feature engineering.

### Algorithm development & performance

The first aim of my project is to develop a machine learning model which can predict lapse from contextualized geolocation data. To accomplish this, first, several classification candidate machine learning algorithms will be considered and will be trained and assessed using participant-grouped, nested *k*-fold cross-validation. Grouped cross-validation assigns all data from a participant as either held-in or held-out to avoid bias introduced when predicting a participant’s data from their own data. Nested cross-validation uses two nested loops for dividing and holding out folds: an outer loop, where held-out folds serve as test sets for model evaluation; and inner loops, where held-out folds serve as validation sets for model selection. Importantly, these sets are independent, maintaining separation between data used to train the models, select the best models, and evaluate those best models. Therefore, nested cross-validation removes optimization bias from the evaluation of model performance in the test sets and can yield lower variance performance estimates than single test set approaches [@jonathanUseCrossvalidationAssess2000].

The primary performance metric for model selection and evaluation will be area under the Receiver Operating Characteristic Curve (auROC) [@kuhnAppliedPredictiveModeling2018]. auROC indexes the probability that the model will predict a higher score for a randomly selected positive case (lapse) relative to a randomly selected negative case (no lapse). This metric was selected because it 1) combines sensitivity and specificity, which are both important characteristics for clinical implementation; 2) is an aggregate metric across all decision thresholds, which is important because optimal decision thresholds may differ across settings and goals; and 3) is unaffected by class imbalance, which is important for comparing models with differing prediction window widths and levels of class imbalance. The best model configuration will be selected using median auROC across all validation sets. Several secondary performance metrics including sensitivity, specificity, balanced accuracy, positive predictive value (PPV), and negative predictive value (NPV) will also be assessed.

SHAP (SHapley Additive exPlanations) values will be computed as our interpretability metric to identify the relative importance of different features in each final algorithm. SHAP values measure the unique contribution of features in an algorithm's predictions [@lundbergUnifiedApproachInterpreting2017]. SHAP values possess several useful properties including: Additivity (SHAP values for each feature can be computed independently and summed); Efficiency (the sum of SHAP values across features must add up to the difference between predicted and observed outcomes for each observation); Symmetry (SHAP values for two features should be equal if the two features contribute equally to all possible coalitions); and Dummy (a feature that does not change the predicted value in any coalition will have a SHAP value of 0). Highly important features represent relevant, actionable potential antecedents to lapse (and therefore points of intervention) that will be relevant in the future development of a smart DTx. However, these will be descriptive analyses because standard errors or other indices of uncertainty for importance scores are not available for SHAP values.

Finally, a Bayesian hierarchical generalized linear model will be used to estimate the posterior probability distributions and 95% Bayesian confidence intervals (CIs) for auROC between the baseline (time-of-day) and geolocation models.

### Algorithmic fairness

The second aim of my project will be to assess the algorithmic fairness of the best-performing model. Several fairness metrics will be assessed across demographic characteristics implicated in treatment disparities, such as the true positive rate (TPR), true negative rate (TNR), and accuracy. These metrics can be calculated using a number of open source R packages [for example, @wisniewskiFairmodelsFlexibleTool2022]. Differences in classification are calculated by first splitting the sample by group, where one is the privileged group and the other is the unprivileged group, and then examining assigned outcomes (in this case, *lapse* or *no lapse*). Additionally, I propose to conduct further analyses in order to assess algorithmic fairness taking into account the intersection of multiple identities when examining model performance [e.g., @fouldsIntersectionalDefinitionFairness2020]. A Bayesian hierarchical generalized linear model will be used to estimate the posterior probability distributions and 95% Bayesian CIs for auROC across different subgroups.

### Contribution to Theory

If successful, this study will contribute both to our ability to predict, and therefore our understanding of, lapse in AUD as well as how we approach designing equitable machine learning models. First, it will identify the most predictive features from a minimally burdensome and continuously collected data source, geolocation data. These features will be created leveraging modeling techniques from both the substance use and affective science subdisciplines, resulting in broadly relevant transdiagnostic constructs. Furthermore, this study will also provide a critical evaluation of model performance beyond AUC by including examination of algorithmic fairness. Though many fairness metrics have been used in simulated data or in publicly available data sets (e.g., *COMPAS* recidivism data set; [@dresselAccuracyFairnessLimits2018]), few researchers in applied mental health research have made use of these tools to critically examine their own models [@timmonsCallActionAssessing2023]. Thus, this study will also advocate for transparency in reporting fairness metrics alongside standard measures of model performance.

## References

::: {#refs}
:::



