---
title: The Feasibility and Equity of Geolocation Data for Lapse Prediction in AUD
author:
    #  - Investigation
  - name: Claire Punturieri 
    email: punturieri@wisc.edu
    #orcid: 0009-0003-7743-3736
    #corresponding: false
    #roles: []
    #affiliations:
      #- Department of Psychology, University of Wisconsin-Madison
  - name: John J. Curtin 
    #orcid: 0000-0002-3286-938X
    #corresponding: true
    email: jjcurtin@wisc.edu
    #roles:
      #- Project administration
      #- Software
      #- Visualization
    #affiliations:
      #- Department of Psychology, University of Wisconsin-Madison 
#keywords:
  #- Substance use disorders
  #- Precision mental health 
#abstract: |
  #To be filled in.
#plain-language-summary: |
  #To be filled in.
#key-points:
  #- Take away point 1 
  #- Take away point 2
date: last-modified
bibliography: references.bib
#citation:
  #container-title: To be filled in. 
number-sections: false 
editor_options: 
  chunk_output_type: console
---

## Introduction

About 1 in 10 adults in the United States met diagnostic criteria for alcohol use disorder (AUD) in 2022 [@Highlights2022National]. While some individuals will experience natural recovery (i.e., improvement without intervention) [@tuckerEpidemiologyRecoveryAlcohol2020], for others AUD will present as a chronic, relapsing disorder marked by periods of recovery interspersed with returns back to harmful use [@ScienceDrugUse; @brandonRelapseRelapsePrevention2007a]. For such individuals, continued monitoring may be beneficial in assisting with the maintenance of recovery goals and in identifying precipitants to lapses, or single instances of goal-inconsistent use that may lead to relapse [@witkiewitzRelapsePreventionAlcohol2004a]. One sustainable and scalable way to provide this continuous monitoring to individuals who need it most is through developing algorithms to predict lapses using both personal sensing data and machine learning.

Personal sensing data are data derived via embedded sensors in technology ubiquitous in our daily lives, such as smartphones, smartwatches, or other wearables [@mohrPersonalSensingUnderstanding2017a]. Because these devices are already so integrated within our day-to-day lives, one benefit of porting these data to clinical use is their proven ability to be collected unobtrusively and continuously. Importantly, these data do not require individuals to change their behavior or routines in any way. Moreover, when paired with machine learning models, statistical patterns connecting antecedents to lapse derived from these data (e.g., changes in mood, difficulty with close social connections, proximity to risky locations) to true lapse events can be uncovered. This is crucial for several reasons: 1) even when someone anticipates an oncoming lapse, it may be difficult to pinpoint the specific driving forces behind it; 2) these precipitating factors will have great variation both between- and within-people; and 3) uncovering these factors may help relieve some of the cognitive burden of recovery (i.e., constant monitoring of potential environmental risk factors).

### Geolocation Data for Risk Monitoring

Recovery and return to use are dynamic processes. Factors that contribute both to maintenance of recovery and return to use change from person-to-person and from moment-to-moment. A shift in social supports (e.g., a move, a break-up) may precede a lapse for one individual but not another. Time spent in locations where alcohol is available (e.g., bars, restaurants, concert venues) may precede a given lapse for another individual, but will not necessarily precede future lapses in that same individual. In order to best capture this fluidity, the ideal data type used within continuous risk monitoring systems should be able to provide a correspondingly appropriate level of granularity. One promising data source is geolocation data.

Geolocation data consist of latitude and longtitude coordinates and can be sampled at regular intervals using applications on smartphones with little to no input from the user beyond initial set-up. Many smartphones and smartwatches automatically collect these data by default. This fact, paired with increasing rates of smartphone ownership, suggest that there is high potential for these data to be feasibly harnessed for use in a risk-monitoring system [@areanMobileTechnologyMental2016]. The importance of location, such as environmental cues or one's perceived riskiness of a setting, has been shown to play an important role in lapse [@janakPotentEffectEnvironmental2010; @waltonIndividualSocialEnvironmental2003;  @waltonSocialSettingsAddiction1995]. This link with lapse risk has translated into the integration of coping skills that target substance-associated contexts in several treatment strategies like mindfulness-based relapse prevention [@lecocqConsideringDrugAssociatedContexts2020]. These findings underscore not only the potential wealth of information relating to relapse risk that an individual's location can provide, but also demonstrate the proven integration of location information into treatment. Furthermore, geolocation data have been specifically identified as being of particular use in both understanding the precipitants to harmful substance use and its effective treatment [@stahlerGeospatialTechnologyExposome2013].

Within the substance use literature, geolocation data have historically been used to examine risky locations, such as the influence of neighborhood characteristics on use [@epsteinRealtimeTrackingNeighborhood2014a; @kwanUncertaintiesGeographicContext2019] and individual physical proximity to locations of potential or past harmful use such as bars (either estimated using geofencing or user-defined) [@attwoodUsingMobileHealth2017; @carreiroRealizeAnalyzeEngage2021; @gonzalezComparisonSmartphoneApp2015; @gustafsonSmartphoneApplicationSupport2014a; @naughtonContextSensingMobilePhone2016]. Several of the applications implemented in these studies enable real-time notifications about locations to their users (e.g., a pop-up message on a smartphone which reads *"You are entering a high-risk zone"*).

On the other hand, affective scientists have focused instead more closely on factors relating to mood. Geolocation data have been used to estimate loneliness and isolation [@doryabIdentifyingBehavioralPhenotypes2019a], to demonstrate increases in positive affect from seeking out novel environments [@hellerAssociationRealworldExperiential2020], and to quantify depressive symptoms [@raughGeolocationDigitalPhenotyping2020]. Moreover, these data have not only been harnessed to measure mood symptoms, but to also predict their emergence [for review, see @shinSystematicReviewLocation2023].

An integration across these subfields can be in part accomplished by enriching geolocation data with brief, intermittent surveys probing specific information about frequently visited locations. For example, some of the more nuanced facets captured within location are associations with others (or lack thereof, e.g., social isolation), associations with previous drinking behaviors (e.g., whether or not alcohol is present), and associations with affect (i.e., negative versus positive emotions tied to a given location).

<!--needs to be a better link here, pending addition of fairness analyses-->

### Model Evaluation

Data selection, however, is only one component of the successful development of a continuous risk monitoring algorithm. Following algorithm development, it is imperative that these models be rigorously evaluated using performance metrics and eventually tested using independent observations (i.e., using data from individuals which were not used in model development). This workflow in machine learning is what enables researchers to anticipate how well a model could be expected to generalize to new populations and is key when aiming to develop algorithms for real-world healthcare implementation. While standard performance metrics like model accuracy, for example, have been standard reporting practice for years, recent literature has begun to urge researchers to also include assessments of how *fair* a model is [@rajkomarEnsuringFairnessMachine2018a; @wawiragichoyaEquityEssenceCall2021]. A fair algorithm is one with no preference in performance with respect to inherent or acquired characteristics (e.g., gender, race, socioeconomic status; [@wangBriefReviewAlgorithmic2022]). In the context of a continuous risk monitoring algorithm for AUD, this would mean that lapse predictions are reasonably accurate and do not favor or disadvantage any particular group.

The motivating factors behind this call to action are clear. In the broader context of health-related data, historical patterns of health care inequities will almost certainly and unavoidably be embedded within data used to train algorithms. These inequities may unintentionally be carried forward in perpetuity by machine learning models if not critically examined. Without examining algorithmic fairness prior to deployment in the real-world, monitoring algorithms run the risk of providing sub-optimal mental health care to individuals who already face disadvantages.

For example, having a limited number of observations within underrepresented groups means that our models will not have as wide a range of individuals to learn from for making predictions of lapse as compared to white, non-Hispanic participants. Performance of these models for racialized minority individuals may therefore be less accurate as a result, particularly without the use of resampling techniques to amend these imbalances [@japkowiczClassImbalanceProblem2000; @wangIntersectionalityMachineLearning2022].

Fairness is also a particular concern specifically in the context of AUD,  where the literature has historically been built upon research developed with male, predominantly white, participants. Despite the call to action brought forth by the NIH through their *Guidelines on Inclusion of Women and Minorities in Research*, recent work has highlighted that seminal research in the field on medications for the treatment of AUD have failed to consistently report participant demographics [@schickCallActionSystematic2020]. This lack of reporting makes it difficult to assess how and if this lack of representation is being corrected. By the very nature of its historically limited participant pool, AUD research and its theory have been developed from a particular perspective using a particular group of individuals. This means that the variables that researchers decide are important to measure and input into models, informed by knowledge of AUD theory, will inherently be biased and may favor these groups. Therefore, researchers can also not assume that balanced classes are enough to compensate for biases brought on by the broader societal context. Both of these facts motivate the reasoning behind examining algorithmic fairness in the context of developing continuous risk monitoring systems.

### The Current Study

In order for these continuous risk monitoring systems to be implemented in the real-world, these models must both be developed outright and rigorously evaluated on both standard performance metrics and algorithmic fairness. To this end, this study utilized geolocation data collected from smartphones and corresponding self-reported contextual information for frequently visited locations to build a machine learning model to predict next-day alcohol use lapse among individuals with a diagnosis of AUD and a recovery goal of abstinence. Model features were engineered from both raw geolocation data, both context-dependent and context agnostic, and change in these data over the previous 6, 12, 24, 48, 72, and 168 hours.

Here we present characterization of model performance for this prediction model in a validation set. We also evaluated feature importance and model fairness. <!--will need to circle back to this and expand--> This study constitutes a preliminary evaluation of a model designed to predict lapse back to alcohol use using minimally burdensome data that has the potential to be integrated within a continuous risk monitoring platform.

## Methods

### Participants

One hundred and forty six individuals in early-recovery (1-8 weeks of abstinence) for AUD were recruited from the Madison area to take part in a three-month study on how mobile health technology can provide recovery support between 2017 and 2019 (R01 AA024391). Recruitment approaches included social media platforms (e.g., Facebook), television and radio advertisements, and clinic referrals. Prospective participants completed a phone screen to assess match with eligibility criteria (Table 1). Participants were excluded if they exhibited severe symptoms of paranoia or psychosis (a score \<= 2.24 on the SCL-90 psychosis scale or a score \<= 2.82 on the SCL-90 paranoia scale administered at screening).

| Eligibility Criteria                                           |
|----------------------------------------------------------------|
| \>= 18 years of age                                            |
| Ability to read and write in English                           |
| Diagnosis of moderate AUD (\>= 4 self-reported DSM-5 symptoms) |
| Abstinent from alcohol for 1-8 weeks                           |
| Willing to use only one smartphone\*\* while on study          |

: Eligibility criteria for study enrollment. \*\*Personal or study-provided.

### Procedure

Participants enrolled in a three-month study consisting of five in-person visits, daily surveys, and continuous passive monitoring of geolocation data. Following screening and enrollment visits in which participants consented to participate, learned how to manage location sharing (i.e., turn off location sharing when desired), and reported frequently visited locations, participants completed three follow-up visits one month apart. At each visit, participants were asked questions about frequently visited (\>2 times during the course of the previous month) locations. Participants were debriefed at the third and final follow-up visit. Participants were expected to provide continuous geolocation data while on study. Other personal sensing data streams (EMA, cellular communications, sleep quality, and audio check-ins) were collected as part of the parent grant’s aims (R01 AA024391).

### Geolocation data

To enable collection of geolocation data, participants downloaded either the Moves app or the FollowMee app during the intake visit. Moves was bought-out and subsequently deprecated while the study was ongoing (July 2018) and data collection continued using FollowMee until the end of the study. Both apps continuously tracked location via GPS and WiFi positioning technology.

Data were then processed to filter out duplicated points, fast movement speeds (\>100mph), sudden positional jumps, and periods of long duration suggesting sampling error issues (\>24 hours with no movement or \>2 hours with a positional jump of more than 0.31 miles or 500 meters). Data points were classified as "in transit" when spacing between individual positions suggested a movement speed of greater than 4mph per NIH health guidelines [@PhysicalActivityGuidelines].

### Contextual information

Contextual information for frequently visited locations (\>2 times in the previous month) was obtained during an interview at each follow-up visit (at month 1, 2, and 3; Table 2). Participants were considered to be at a known contextual location if they were within 0.031 miles (50 meters) of a reported frequently visited location.

| Question                                                                                                      | Responses                                                                                                                                                                                                                                                                                      |
|-----------------------|-------------------------------------------------|
| Address                                                                                                       |                                                                                                                                                                                                                                                                                                |
| Type of place                                                                                                 | Work, School, Volunteer, Health care, Home of a friend, Home of a family member, Liquor store, Errands (e.g., grocery store, post office), Coffee shop or cafe, Restaurant, Park, Bar, Gym or fitness center, AA or recovery meeting, Religious location (e.g., church, mosque, temple), Other |
| Have you drank alcohol here before?                                                                           | No, Yes                                                                                                                                                                                                                                                                                        |
| Is alcohol available here?                                                                                    | No, Yes                                                                                                                                                                                                                                                                                        |
| How would you describe your experiences here?                                                                 | Pleasant, Unpleasant, Mixed, Neutral                                                                                                                                                                                                                                                           |
| Does being at this location put you at any risk to begin drinking?                                            | No risk, Low risk, Medium risk, High risk                                                                                                                                                                                                                                                      |
| Did the participant identify this place as a risky location they are trying to avoid now that they are sober? | No, Yes                                                                                                                                                                                                                                                                                        |

: Location information collected from frequently visited locations.

### Participant characteristics

Participants completed a baseline measure of demographics and other constructs relevant to lapse at the screening visit, which was used for fairness assessments (Table 3).

| Variable     | Measure                                                           |
|--------------------------|----------------------------------------------|
| Demographics | Age                                                               |
|              | Sex                                                               |
|              | Race                                                              |
|              | Ethnicity                                                         |
|              | Employment                                                        |
|              | Income                                                            |
|              | Marital Status                                                    |
| Alcohol      | Alcohol Use History                                               |
|              | DSM-5 Checklist for AUD                                           |
|              | Young Adult Alcohol Problems Test                                 |
|              | WHO-The Alcohol, Smoking and Substance Involvement Screening Test |

: Demographic and relevant alcohol use history variables sampled at screening visit.

### Lapses

Alcohol lapses were used as the outcome variable in this study and were used to provide labels for model training, for testing model performance, and for testing issues of algorithmic fairness across our predefined subgroups. Future lapse occurrence (here conceptualized as next-day lapse) was be predicted in 24-hour windows, beginning at 4:00am on a participant's second day of participation to ensure one full day of data collection for the first window, and at every subsequent day on study thereafter. *Lapse* and *no lapse* occurrences were identified from the daily survey question, *"Have you drank any alcohol that you have not yet reported?"*. Participants who responded *yes* to this question were then asked to report the date and hour of the start and the end of the drinking episode. In this case, the prediction window was labeled *lapse*. Prediction windows were labeled *no lapse* if no alcohol use was reported within that window.

### Feature engineering

Feature engineering is the process of creating variables (or *"features"*) from unprocessed data and was used to transform raw data from geolocation data collected the prior day.

Features from geolocation data were generated that utilized both contextual information collected from monthly surveys (e.g., location valence, perceived riskiness) as well as features that were independent of further individual input (e.g., location variance, time spent out of the home in the evening). <!--list out all features? maybe make table?--> All features were calculated both as raw and change features based on previous geolocation data (i.e., change from past 6, 12, 24, 48, 72, and 168 hour periods) in order to capture individual variation.

Imputation of missing data and removal of zero-variance features are additional general processing steps that will also be undertaken during feature engineering.

### Algorithm development & performance

<!--worthwhile to mention glmnet?-->

Several configurations of the XGBoost machine learning algorithm were considered which varied across a relevant and appropriate range of model-specific hyperparameters (mtry, tree depth, learning rate) as well as resampling techniques (up-sampling of the positive class, lapse, and down-sampling of the negative class, no lapse).

Models were trained and assessed using participant-grouped, nested *k*-fold cross-validation. Grouped cross-validation assigns all data from a participant as either held-in or held-out to avoid bias introduced when predicting a participant’s data from their own data. Nested cross-validation uses two nested loops for dividing and holding out folds: an outer loop, where held-out folds serve as test sets for model evaluation; and inner loops, where held-out folds serve as validation sets for model selection. Importantly, these sets are independent, maintaining separation between data used to train the models, select the best models, and evaluate those best models. Therefore, nested cross-validation removes optimization bias from the evaluation of model performance in the test sets and can yield lower variance performance estimates than single test set approaches [@jonathanUseCrossvalidationAssess2000].

The primary performance metric for model selection and evaluation of the validation set was area under the Receiver Operating Characteristic Curve (auROC) [@kuhnAppliedPredictiveModeling2018]. auROC indexes the probability that the model will predict a higher score for a randomly selected positive case (lapse) relative to a randomly selected negative case (no lapse). This metric was selected because it 1) combines sensitivity and specificity, which are both important characteristics for clinical implementation; 2) is an aggregate metric across all decision thresholds, which is important because optimal decision thresholds may differ across settings and goals; and 3) is unaffected by class imbalance, which is important for comparing models with differing prediction window widths and levels of class imbalance. The best model configuration was selected using median auROC across all validation sets. Several secondary performance metrics including sensitivity, specificity, balanced accuracy, positive predictive value (PPV), and negative predictive value (NPV) will also be assessed.

SHAP (SHapley Additive exPlanations) values were computed as interpretability metrics to identify the relative importance of different features in each final algorithm. SHAP values measure the unique contribution of features in an algorithm's predictions [@lundbergUnifiedApproachInterpreting2017]. SHAP values possess several useful properties including: *Additivity* (SHAP values for each feature can be computed independently and summed); *Efficiency* (the sum of SHAP values across features must add up to the difference between predicted and observed outcomes for each observation); *Symmetry* (SHAP values for two features should be equal if the two features contribute equally to all possible coalitions); and *Dummy* (a feature that does not change the predicted value in any coalition will have a SHAP value of 0). Highly important features represent relevant, actionable potential antecedents to lapse (and therefore points of intervention) that will be relevant in the future development of a continuous risk monitoring system. However, these are descriptive analyses because standard errors or other indices of uncertainty for importance scores are not available for SHAP values.

Finally, a Bayesian hierarchical generalized linear model was used to estimate the posterior probability distributions and 95% Bayesian credible intervals (CIs) for auROC.

### Algorithmic fairness

<!--circle back to this-->

Subgroups were defined on the basis of personal individual characteristics (in the machine learning fairness literature, "sensitive attributes") that are specifically associated with treatment disparities in AUD.

A Bayesian hierarchical generalized linear model was used to estimate the posterior probability distributions and 95% Bayesian CIs for auROC across different subgroups.

## Results

### Demographics

A total of 192 individuals were eligible to participate in the study, of which 191 consented to participate and 169 enrolled in the study. Fifteen participants were excluded prior to the first monthly follow-up visit. One participant was excluded for not maintaining a recovery goal of abstinence during their time on study. Two participants were excluded due to evidence of low compliance and careless responding. A further five individuals were excluded due to poor geolocation data quality as a result of insufficient data (primarily resulting from software incompatibility), resulting in a final sample size of 146.

The average age of the final sample was 40.9 years (SD = 12 years, range = 21-72 years). There was an approximately equal number of men (n = 74, 50.7%) and women (n = 72, 49.3%). The majority of the sample was White/Caucasian (n = 127, 86.99%) and non-Hispanic (n = 142, n = 97%). The mean income of participants was \$34,408 (SD = \$32,259, range = \$0-\$200,000). On average, participants self-reported a mean number of 8.9 DSM-V symptoms of AUD (range = 4-11).

<!--table or figures?-->

### Model Evaluation

{{< embed notebooks/auROC_distribution_posterior.qmd#fig-auroc-histogram >}}

{{< embed notebooks/auROC_distribution_posterior.qmd#fig-pp-density >}}

Insert updated auROC plot with new inner preds

Insert calibration plot with new inner preds

global feture importance plot.  This will be features by broad categories


Fairness plots.    auROC in subgroups.


### Feature Importance

### Algorithmic Fairness

## Conclusion

Brief overview goes here. Mention validation set.

### Future directions

- Baseline model?
- Add in more affective features
- Add in risk-terrain modeling features
- Add in other important features that could contribute to movement patterns like day of the week and weather
- Test final model

## References

::: {#refs}
:::



