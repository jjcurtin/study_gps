---
title: "Make enriched gps/location"
author: "John Curtin, updated by Claire Punturieri"
date: "`r lubridate::today()`"
format: 
  html:
    toc: true
    toc_depth: 4
    embed-resources: true
editor_options: 
  chunk_output_type: console
---

# Housekeeping

## Code Status

Completed as of 08/2024. Updated as of 03/2025.

## Notes

This script generates and does EDA to make filtering decisions for GPS **places** features.

Summary of filtering decisions:

- For `dist` greater than 0.01 and `duration` of 0, `duration` will be set to NA
- For `speed` greater than 100mph, `duration`, will be set to NA (assuming this is at/around top car speed)
- For `duration` > 2 hours and `dist` of > 0.31, `duration` will be set to NA
- For `duration` > 24, regardless of `dist`, `duration` will be set to 24

# Set up

## Set up environment

```{r}
#| message: false
#| warning: false

options(conflicts.policy = "depends.ok")
devtools::source_url("https://github.com/jjcurtin/lab_support/blob/main/format_path.R?raw=true", 
                      sha1 = "d1f1c542783f2e9a6ff50a400f909ba175ac618e")
```

## Paths

```{r}
path_shared <- format_path("studydata/risk/data_processed/shared")
```

## Packages and source

```{r}
#| message: false
#| warning: false
#| echo: false

# for data wrangling
library(tidyverse)
library(janitor, exclude = c("chisq.test", 
                             "fisher.test"))
library(lubridate)
library(future)

source(here::here("../lab_support/fun_gps.R"))

# helpful lab functions
devtools::source_url("https://github.com/jjcurtin/lab_support/blob/main/fun_eda.R?raw=true",
 sha1 = "c045eee2655a18dc85e715b78182f176327358a7"
)

# adjust plot visuals
theme_set(theme_classic())
options(tibble.width = Inf, dplyr.print_max = Inf)
```

## Initialize parallel processing

```{r}
cl <- parallel::makePSOCKcluster(parallel::detectCores(logical = FALSE))
doParallel::registerDoParallel(cl)
plan(multisession, workers = parallel::detectCores(logical = FALSE))
```

## Load in data

Create function to filter data based on study_dates. This is important for subjects for whom we only want to use some of their data.

```{r}
date_filter <- function(id, gps, study_dates) {
  gps <- gps |> 
    filter(subid == id)
    
  study_dates <- study_dates |> 
    filter(subid == id)
    
  gps <- gps |> 
    filter(date >= study_dates$study_start) |> 
    filter(date <= study_dates$study_end)

  return(gps)
}
```

Create function to match GPS with context by subid.

```{r}
enrich_subid <- function(a_subid, gps, locs) {
  gps <- gps |>  
    filter(subid == a_subid)
  
  locs <- locs |>  
    filter(subid == a_subid)
  
  enriched <- gps |> 
    bind_cols(
      map2(gps$lon, gps$lat, 
           \(lon, lat) find_nearest_context(lon, lat, context = locs)) |> 
      bind_rows()
    )
  
  return(enriched)
}
```

Generate places data frame.

```{r}
if(file.exists(here::here(path_shared, "gps_enriched.csv.xz"))){

  message("Aggregate file already exist -- delete to recreate!")
  
  places <- read_csv(here::here(path_shared, "gps_enriched.csv.xz"), 
                 show_col_types = FALSE) |>
    mutate(time = with_tz(time, tz = "America/Chicago"),
           dist = dist / 1609.344,
           duration = duration / 60,
           speed = dist / duration,
           dist_context = dist_context / 1609.344)

  # count number of observations per day
  places_obs <- places |>
    group_by(subid, date) |>
    summarise(n_obs = n())

  places <- places_obs |> 
    right_join(places, by = c("subid", "date")) |> 
    ungroup()
  
} else{
  
  message("Merging raw GPS and context information...")
  
  message("...loading study_dates_gps.csv")
  study_dates <- read_csv(here::here(path_shared, "study_dates_gps.csv"),
                          show_col_types = FALSE) |>
    mutate(study_start = as_date(study_start),
           study_end = as_date(study_end))  |>
    mutate(study_ndays = as.numeric(difftime(study_end,
                                             study_start, units = "days")))
  
  subids_dates <- study_dates |>
    pull(subid) |>  
    unique()
  
  message("...loading locations.csv")
  locs <- read_csv(here::here(path_shared, "locations.csv"),
                   col_types = "cddcccccccccccdd",
                   show_col_types = FALSE) |>
    mutate(subid = as.numeric(subid)) |>
    filter(subid %in% subids_dates)
  
  message("...loading gps.csv")
  gps <-  read_csv(here::here(path_shared, "gps.csv"),
                   show_col_types = FALSE)  |>
    filter(subid %in% subids_dates)  |>    # limit to sample in study_dates
    mutate(time = with_tz(time, tz = "America/Chicago")) |>  
    group_by(subid)  |>  
    mutate(date = date(time),
           dist = distGeo(as.matrix(tibble(lon, lat))),
           duration = difftime(lead(time), time, units = "mins"),
           duration = as.numeric(duration)) |> 
    ungroup()
  
  message("...filtering GPS data to selected dates")
  gps <- subids_dates |>
    map(\(id) date_filter(id, gps, study_dates)) |> 
    bind_rows()
  
  subids_gps <- gps |>  
    pull(subid)  |>  
    unique()
  
  message("...creating place dataframe")
  places <- subids_gps  |>
      furrr::future_map_dfr(enrich_subid, gps = gps, locs = locs,
                        .options = furrr::furrr_options(seed = NULL)) |> 
    left_join(locs, by = c("subid", "context_id"))  |>
    select(-lat.y, -lon.y) |>  
    rename(lat = lat.x, lon = lon.x) |>   
    relocate(subid) |> 
    # convert from meters to miles, minutes to hours
    mutate(dist = dist / 1609.344,
           duration = duration / 60,
           speed = dist / duration,
           dist_context = dist_context / 1609.344)
  
  # count number of observations per day
  places_obs <- places |>
    group_by(subid, date) |>
    summarise(n_obs = n())

  places <- places_obs |> 
    right_join(places, by = c("subid", "date")) |> 
    ungroup()
  
}
```

# Filtering

## Overview

Glimpse places data.

```{r}
places |>
  glimpse()
```

Check length of subids (verify it is the same as above).

```{r}
places  |>  
  pull(subid) |>
  unique() |>  
  length()
```

Summarize number of unique `context_id` observations per subject.

```{r}
places |>
  group_by(subid) |>
  summarise(unique_context = n_distinct(context_id)) |> 
  print_kbl()

places |> 
  group_by(subid) |>
  summarise(unique_context = n_distinct(context_id)) |> 
  summarise(mean(unique_context), min(unique_context), max(unique_context))
```

Display unique `context_id` observations per subject in a histogram.

```{r}
places |>
  group_by(subid) |>
  summarise(unique_context = n_distinct(context_id)) |>
  pull(unique_context) |> 
  hist(col = "white", border = "black",
       xlab = "# of Unique Context IDs", ylab = "Frequency",
       main = "Histogram of Unique Context IDs per Subject")
```

## Decisions

### One observation per day points

Define function to pull days with one GPS observation and the preceding and following row (AKA observation).

```{r}
single_obs <- function(places) {
  inds <- which(places$n_obs == 1)
  rows <- map(inds, function(x) (x-1):(x+1))
  places[unlist(rows),]
}
```

Display number of 1-observation days.

```{r}
places |> filter(n_obs == 1) |> nrow()
```

Print table displaying 1-observation days, and preceding/subsequent rows.

```{r}
places |>
  single_obs() |>
  select(subid, time, n_obs, duration, dist) |>
  print_kbl(digits = 4) |>
  kableExtra::scroll_box(width = "700px", height = "500px")
```

Print table displaying only 1-observation days, descending by `duration`.
Note: time differences calculated with LEAD (difference from NEXT row).

Suspicious if `duration` is LOW. Low observation days should have their point sampled closely to the
beginning of the subsequent day.

```{r}
places |>
  filter(n_obs == 1) |> 
  select(subid, time, n_obs, duration, dist) |>
  mutate(next_time = time + lubridate::minutes(as.integer(duration * 60)),
         date_diff = date(next_time) - date(time)) |>
  arrange(desc(duration)) |> 
  print_kbl(digits = 4) |> 
  kableExtra::scroll_box(width = "700px", height = "500px")
```

Print table displaying only 1-observation days, descending by `dist`.

Suspicious if `dist` is HIGH because it suggests that we are missing observations (participant appears to "jump" to next location).

```{r}
places |>
  filter(n_obs == 1) |> 
  select(subid, date, n_obs, duration, dist) |>
  arrange(desc(dist)) |> 
  print_kbl(digits = 4) |> 
  kableExtra::scroll_box(width = "600px", height = "500px")
```

Count number of occurrences of 1-observation days by `dist`.

```{r}
places |>
  filter(n_obs == 1) |> 
  mutate(dist = round(dist, 2)) |> 
  group_by(dist) |>
  summarise(n_dist = n()) |> 
  arrange(desc(dist)) |> 
  print_kbl()
```

Histogram of `dist` observations less than 2 miles for 1-observation days.

```{r}
places |>
  filter(n_obs == 1) |> 
  mutate(dist = round(dist, 2)) |> 
  group_by(dist) |>
  summarise(n_dist = n()) |>
  filter(dist < 2) |>
  pull(dist) |> 
  hist(breaks = 30)
```

Histogram of `dist` observations greater than 2 miles for 1-observation days.
Note: the only point excluded with the 50  mile filtering is an observation
that is 800 miles away.

```{r}
places |>
  filter(n_obs == 1) |> 
  select(subid, date, n_obs, duration, dist) |>
  # mask 800 mile point
  filter(dist < 50 & dist > 2) |> 
  pull(dist) |> 
  hist(breaks = 70)
```

**FILTERING DECISION**

If distance is more than 500 meters (0.31 miles), set duration to NA because we cannot trust the duration of that observation. However, this will be addressed in the *Extended Periods* section, so no filtering will be applied here.

### Speed

#### Speed NaN

`speed` is NaN if `dist` and `duration` are both equal to 0, or if `dist` and `duration` are NaN to begin with.

Generate table of instances where `speed` is NaN.

```{r}
places |>
  filter(is.na(speed)) |>
  select(subid, dist, duration, speed) |> 
  print_kbl() |> 
  kableExtra::scroll_box(width = "400px", height = "500px")
```

Count number of observations where `speed` is NaN.

```{r}
places |> 
  filter(is.na(speed)) |> 
  nrow()
```

Check points before `dist` and `duration` = 0 to see if `lat` and `long` are the same.

```{r}
speed_nan <- function(places) {
  inds <- which(places$dist == 0 & places$duration == 0)
  rows <- map(inds, function(x) (x-1):(x+1))
  places[unlist(rows),]
}
```

These points appear to be the same, suggesting erroneous rapid resampling (duplicate points!).

```{r}
places |>
  select(subid, lat, lon, dist, duration, speed) |> 
  speed_nan() |>
  # subset for easy loading
  head(300) |> 
  print_kbl(digits = 4) |> 
  kableExtra::scroll_box(width = "500px", height = "500px")
```

#### Speed Inf

`speed` is infinite if there is a value for `dist` but no value for `duration`.

Generate table displaying instances of Inf `speed`.

```{r}
places |>
  filter(is.infinite(speed)) |>
  select(subid, dist, duration, speed) |>
  arrange(desc(dist)) |> 
  print_kbl(digits = 4) |> 
  kableExtra::scroll_box(width = "500px", height = "500px")
```

About half of infinite points are due to a very low distance and 0 duration (0.01 miles = ~16 meters ~50 feet).
Distance jumps greater than this amount with 0 duration are suggestive of a sampling error and
the duration at this point is therefore not trustworthy.

Count number of observations less than 0.01 miles from next point.

```{r}
places |> 
  filter(is.infinite(speed)) |> 
  filter(dist < 0.01) |> 
  nrow()
```

Count number of observations greater than 0.01 miles from next point.

```{r}
places |> 
  filter(is.infinite(speed)) |> 
  filter(dist > 0.01) |> 
  nrow()
```

Check points before `dist` < 0.01 and `duration` = 0 to see if `lat` and `long` are the same.

```{r}
speed_inf_lowdist <- function(places) {
  inds <- which(places$dist < 0.01 & places$duration == 0 & is.infinite(places$speed))
  rows <- map(inds, function(x) (x-1):(x+1))
  places[unlist(rows),]
}
```

Like our NaN points, these also appear to match the subsequent point.
This suggests that these instances of "infinite" `speed` are actually caused by rapidly repeated
sampling of a time point.

```{r}
places |>
  select(subid, time, lat, lon, dist, duration, speed) |> 
  speed_inf_lowdist() |>
  # subset for easy loading
  head(300) |> 
  print_kbl(digits = 4) |> 
  kableExtra::scroll_box(width = "600px", height = "500px")
```

#### NaN and Inf speed by data type

There does not appear to be an impact of `data_type` (GPS or network) on Inf or NaN points.

Create tibble of `data_type` when `speed` is NaN.

```{r}
places |>
  filter(is.nan(speed)) |> 
  tab(data_type)
```

Create tibble of `data_type` when `speed` is Inf.

```{r}
places |>
  filter(is.infinite(speed)) |> 
  tab(data_type)
```

Count number of observations of `data_type` other than GPS (AKA network).

```{r}
places |>
  filter(!(data_type == "gps")) |> 
  nrow()
```

Print table displaying observations for which `data_type` is not GPS (AKA network).

```{r}
places |> 
  filter(!(data_type == "gps")) |> 
  select(subid, lat, lon, dist, duration, speed, data_type) |>
  arrange(desc(speed)) |> 
  print_kbl() |> 
  kableExtra::scroll_box(width = "500px", height = "500px")
```

**FILTERING DECISION**

We will LEAVE points where `dist` and `duration` are equal to 0, or where `dist` is less than 0.01 and `duration` is 0.
For `dist` greater than 0.01 and `duration` of 0, we will set `duration` to NA.

Check the number of points whose `duration` will be set to NA.

```{r}
places |>
  filter(dist > 0.01 & duration == 0) |> 
  nrow()
```

Display table with points that will be set to NA (`dist` > 0.01 miles and `duration` = 0)
to confirm that they are indeed being set to NA.

```{r}
places |>
    filter(dist > 0.01 & duration == 0) |> 
    mutate(duration = if_else(dist > 0.01 & duration == 0, NA_real_,
                            duration)) |>
  select(subid, lat, lon, dist, duration, speed) |> 
  print_kbl() |> 
  kableExtra::scroll_box(width = "500px", height = "500px")
```

#### Improbable speeds

Improbable speed is defined as > 100mph, which is at/around realistic maximum car speed.

Generate histogram of all speed values.

```{r}
places |> 
  mutate(duration = if_else(dist > 0.01 & duration == 0, NA_real_,
                            duration)) |>
  filter(!(is.na(duration))) |> 
  pull(speed) |> 
  hist()
```

Generate histogram of speed under 100mph.

```{r}
places |> 
  mutate(duration = if_else(dist > 0.01 & duration == 0, NA_real_,
                            duration)) |>
  filter(!(is.na(duration))) |>
  filter(speed < 100) |> 
  pull(speed) |> 
  hist()
```

Count number of observations **under 100mph.**

```{r}
places |> 
 mutate(duration = if_else(dist > 0.01 & duration == 0, NA_real_,
                            duration)) |>
  filter(!(is.na(duration))) |>
  filter(speed < 100) |> 
  nrow()
```

Count number of observations **above 100mph.**

```{r}
places |> 
  mutate(duration = if_else(dist > 0.01 & duration == 0, NA_real_,
                            duration)) |>
  filter(!(is.na(duration))) |>
  filter(speed > 100) |> 
  nrow()
```

Generate table of speeds over 100mph.

```{r}
places |> 
  mutate(duration = if_else(dist > 0.01 & duration == 0, NA_real_,
                            duration)) |>
  filter(speed > 100) |>
  select(subid, lat, lon, dist, duration, speed) |>
  arrange(desc(speed)) |> 
  print_kbl() |> 
  kableExtra::scroll_box(width = "500px", height = "500px")
```

**Two types of potential speed issues to look at:**
- Short distance and short duration
- Long distance and short duration

##### Short distance, short duration

High values for `speed` resulting from small `dist` and `duration` values might be because of rapid resampling.

```{r}
places |> 
  mutate(duration = if_else(dist > 0.01 & duration == 0, NA_real_,
                            duration)) |>
  filter(!(is.infinite(speed))) |>
  filter(speed > 100) |>
  # less than 500 meters and 5 minutes
  filter(dist < 0.31 & duration < 0.0833) |> 
  select(subid, lat, lon, dist, duration, speed) |>
  arrange(desc(speed)) |>
  print_kbl(digits = 4) |>
  kableExtra::scroll_box(width = "500px", height = "500px")
```

Count number of observations of high `speed` with small `dist` and `duration` values.

```{r}
places |> 
  mutate(duration = if_else(dist > 0.01 & duration == 0, NA_real_,
                            duration)) |>
  filter(!(is.na(duration))) |>
  filter(speed > 100) |>
  # less than 500 meters and 5 minutes
  filter(dist < 0.31 & duration < 0.0833) |> 
  nrow()
```

##### Long distance, short duration

Possible contributors to high `speed` values with high `dist` and low `duration`
values are very fast travel and/or sampling error.

```{r}
places |> 
  mutate(duration = if_else(dist > 0.01 & duration == 0, NA_real_,
                            duration)) |>
  filter(speed > 100) |>
  # distance of more than 500m and less than 5 minutes
  filter(dist > 0.31 & duration < 0.0833) |> 
  select(subid, lat, lon, dist, duration, speed) |>
  arrange(desc(dist)) |>
  print_kbl(digits = 4) |>
  kableExtra::scroll_box(width = "500px", height = "500px")
```

Count number of observations with high `speed` due to high `dist` and low `duration`.

```{r}
places |> 
  mutate(duration = if_else(dist > 0.01 & duration == 0, NA_real_,
                            duration)) |>
  filter(!(is.na(duration))) |>
  filter(speed > 100) |>
  # distance of more than 500m and less than 5 minutes
  filter(dist > 0.31 & duration < 0.0833) |> 
  nrow()
```

**FILTERING DECISION**

Set `duration` to NA for all observations above 100mph (assuming this is at or around top car speed).

Count number of observations for which `duration` will be set to NA.

```{r}
places |> 
  mutate(duration = if_else(dist > 0.01 & duration == 0, NA_real_,
                            duration)) |>
  filter(!(is.na(duration))) |>
  filter(speed > 100) |> 
  nrow()
```

Confirm that the `duration` value of these points is correctly set to NA.

```{r}
places |>
  filter(speed > 100) |> 
  mutate(duration = if_else(dist > 0.01 & duration == 0, NA_real_,
                            duration),
         duration = if_else(speed > 100, NA_real_,
                            duration)) |>
  select(subid, lat, lon, dist, duration, speed) |>
  arrange(desc(dist)) |>
  head(300) |> 
  print_kbl(digits = 4) |>
  kableExtra::scroll_box(width = "500px", height = "500px")
```

### Long time at point

#### Extended periods

Check points that have a suspiciously long duration.
Note that the forced resampling rate for FollowMee was ~2 hours, but is unknown for Moves.
(See sop_android_tech_setup_followmee in proj_risk/methods/sop)

Generate table to examine points with a `duration` value of longer than 2 hours.

```{r}
places |>
  mutate(duration = if_else(dist > 0.01 & duration == 0, NA_real_,
                            duration),
         duration = if_else(speed > 100, NA_real_,
                            duration)) |>
  filter(duration > 2) |>
  select(subid, dist, duration) |> 
  arrange(desc(dist)) |> 
  print_kbl() |> 
  kableExtra::scroll_box(width = "500px", height = "500px")
```

Examine long `duration` counts by app source (Moves or FollowMee).

```{r}
places |>
  mutate(duration = if_else(dist > 0.01 & duration == 0, NA_real_,
                            duration),
         duration = if_else(speed > 100, NA_real_,
                            duration)) |>
  filter(duration > 2) |>
  tab(app_source)
```

Examine `duration` quantiles.

```{r}
places |> 
  mutate(duration = if_else(dist > 0.01 & duration == 0, NA_real_,
                            duration),
         duration = if_else(speed > 100, NA_real_,
                            duration)) |>
  pull(duration) |> 
  quantile(c(.1, .25, .5, .75, .9, .99, .999), na.rm = TRUE)
```

Generate histogram displaying duration in hours.

```{r}
places |>
  mutate(duration = if_else(dist > 0.01 & duration == 0, NA_real_,
                            duration),
         duration = if_else(speed > 100, NA_real_,
                            duration)) |>
  pull(duration) |> 
  hist()
```

Generate histogram for `duration` values greater than 48 hours.

```{r}
places |>
  mutate(duration = if_else(dist > 0.01 & duration == 0, NA_real_,
                            duration),
         duration = if_else(speed > 100, NA_real_,
                            duration)) |>
  filter(duration > 48) |>
  pull(duration) |> 
  hist(breaks = 70)
```

Generate histogram for `duration` values less than 48 hours.

```{r}
places |>
  mutate(duration = if_else(dist > 0.01 & duration == 0, NA_real_,
                            duration),
         duration = if_else(speed > 100, NA_real_,
                            duration)) |>
  filter(duration < 48) |>
  pull(duration) |> 
  hist(breaks = 70)
```

Count number of points with a duration greater than 48 hours.

```{r}
places |> 
  mutate(duration = if_else(dist > 0.01 & duration == 0, NA_real_,
                            duration),
         duration = if_else(speed > 100, NA_real_,
                            duration)) |>
  filter(duration > 48) |> 
  nrow()
```

Generate table to examine points with a duration of greater than 48 hours.
Possible reasons for missing data could be tech issues or not leaving the house.

*Note that CP originally cross-referenced these participants with study_notes, but CP and JC
ultimately decided that it would be better to filter data without using study_notes knowledge
to more appropriately simulate handling of data in a real-world implementation (i.e., we would
not have access to notes about who stays home more versus who has tech issues).*

```{r}
places |>
  mutate(duration = if_else(dist > 0.01 & duration == 0, NA_real_,
                            duration),
         duration = if_else(speed > 100, NA_real_,
                            duration)) |>
  filter(duration > 48) |>
  select(subid, date, duration) |> 
  print_kbl() |> 
  kableExtra::scroll_box(width = "500px", height = "500px")
```

It is also important to consider **where** folks are at when these long gaps of time are appearing. Count number of long duration periods NOT at a known location, at a known location, and display WHERE these known locations are.

```{r}
places |>
  mutate(duration = if_else(dist > 0.01 & duration == 0, NA_real_,
                          duration),
        duration = if_else(speed > 100, NA_real_,
                          duration)) |>
  filter(duration > 24) |>
  mutate(known_loc = if_else(dist_context <= 0.031 & speed <= 4, "yes", "no"),
         known_loc = if_else(is.na(known_loc), "no", known_loc)) |> 
  select(subid, time, duration, known_loc, type) |> 
  distinct(time, .keep_all = TRUE) |> 
  tabyl(known_loc)
```

```{r}
places |>
  mutate(duration = if_else(dist > 0.01 & duration == 0, NA_real_,
                          duration),
         duration = if_else(speed > 100, NA_real_,
                          duration)) |>
  filter(duration > 24) |>
  mutate(known_loc = if_else(dist_context <= 0.031 & speed <= 4, "yes", "no"),
         known_loc = if_else(is.na(known_loc), "no", known_loc)) |> 
  select(subid, time, duration, known_loc, type) |> 
  distinct(time, .keep_all = TRUE) |> 
  filter(known_loc == "yes") |>
  select(subid, duration, type) |> 
  tabyl(type) |> 
  arrange(desc(n))
```

**FILTERING DECISION**

Set a tolerance level for amount of extended periods of no data we tolerate.

Because the forced sampling rate for FollowMee is 2 hours, in a 2 hour period we would
expect to get at least one GPS point. Therefore, for a duration > 2 hours and a distance
of > 0.31, we will set the duration to NA. For a duration > 24, regardless of
distance, the duration will be set to 24.

This enables us to capture some information about extended stays for participants who were more
sedentary because it retains long `duration` if an individual stayed in the same location.

Count number of points with a duration greater than 2 and a distance greater
than 0.31.

```{r}
places |>
  mutate(duration = if_else(dist > 0.01 & duration == 0, NA_real_,
                            duration),
         duration = if_else(speed > 100, NA_real_,
                            duration)) |>
  filter(duration > 2 & dist > 0.31) |>
  nrow()
```

Count number of points with a duration greater than 24, regardless of distance.

```{r}
places |>
  mutate(duration = if_else(dist > 0.01 & duration == 0, NA_real_,
                            duration),
         duration = if_else(speed > 100, NA_real_,
                            duration)) |>
  filter(!(duration > 2 & dist > 0.31)) |>
  filter(duration > 24) |> view()
 # nrow()
```

Create tables to ensure that we are successfully amending duration values for these points.

```{r}
places |>
  filter(duration > 2 & dist > 0.31) |> 
  mutate(duration = if_else(dist > 0.01 & duration == 0, NA_real_,
                            duration),
         duration = if_else(speed > 100, NA_real_,
                            duration),
         duration = if_else(duration > 2 & dist > 0.31, NA_real_,
                            duration),
         duration = if_else(duration > 24, 24,
                            duration)) |>
  select(subid, lat, lon, dist, duration, speed) |>
  head(300) |> 
  print_kbl(digits = 4) |>
  kableExtra::scroll_box(width = "500px", height = "500px")
```

```{r}
places |>
  filter(duration > 24) |> 
  mutate(duration = if_else(dist > 0.01 & duration == 0, NA_real_,
                            duration),
         duration = if_else(speed > 100, NA_real_,
                            duration),
         duration = if_else(duration > 2 & dist > 0.31, NA_real_,
                            duration),
         duration = if_else(duration > 24, 24,
                            duration)) |>
  select(subid, lat, lon, dist, duration, speed) |>
  head(300) |> 
  print_kbl(digits = 4) |>
  kableExtra::scroll_box(width = "500px", height = "500px")
```

### Next points far away

Create histogram of `dist` with updated filtering.

```{r}
places |>
  mutate(duration = if_else(dist > 0.01 & duration == 0, NA_real_,
                            duration),
         duration = if_else(speed > 100, NA_real_,
                            duration),
         duration = if_else(duration > 2 & dist > 0.31, NA_real_,
                            duration),
         duration = if_else(duration > 24, 24,
                            duration)) |>
  # filter out points that have a missing duration for visualization purposes
  filter(!(is.na(duration))) |> 
  pull(dist) |> 
  hist()
```

Generate table, descending by `dist`, to see if there are still problematic
high value points.

```{r}
places |> 
  mutate(duration = if_else(dist > 0.01 & duration == 0, NA_real_,
                            duration),
         duration = if_else(speed > 100, NA_real_,
                            duration),
         duration = if_else(duration > 2 & dist > 0.31, NA_real_,
                            duration),
         duration = if_else(duration > 24, 24,
                            duration)) |>
  arrange(desc(dist)) |>
  filter(!(is.na(duration))) |> 
  head(600) |>
  select(subid, lat, lon, dist, duration, speed) |>
  print_kbl(digits = 4) |>
  kableExtra::scroll_box(width = "500px", height = "500px")
```

Calculate the percentage `duration` of points less than 24 hours but with a `duration`
value greater than 2 hours and a `dist` greater than 0.31 miles (500 meters).

```{r}
places |> 
  mutate(duration = if_else(dist > 0.01 & duration == 0, NA_real_,
                            duration),
         duration = if_else(speed > 100, NA_real_,
                            duration)) |> 
  filter(!(is.na(duration))) |>
  filter(duration < 24) |>
  mutate(d2 = if_else(duration > 2 & dist > 0.31, 1, 0)) |> 
  pull(d2) |> 
  mean() * 100
```

**FILTERING DECISION**

These high `dist` points make practical sense with the other filtering we have already done.
Longer distances traveled appear to fit reasonably within an expected timeframe.
Therefore, no additional filtering will be done here.

### Classifying stationary/transit behavior

We will classify someone as being at a known if they are within 0.031 miles (50 meters)
of that location.

Display table showing breakdown of points that fall at known versus unknown locations.

```{r}
places |>
  mutate(duration = if_else(dist > 0.01 & duration == 0, NA_real_,
                            duration),
         duration = if_else(speed > 100, NA_real_,
                            duration),
         duration = if_else(duration > 2 & dist > 0.31, NA_real_,
                            duration),
         duration = if_else(duration > 24, 24,
                            duration)) |>
  mutate(known_loc = if_else(dist_context <= 0.031, TRUE, FALSE)) |>
  tabyl(known_loc)
```

Create subset of places dataframe to assess how well we can classify stationary versus transit points.

```{r}
places_sub <- places |>
  # apply previous filtering
  mutate(duration = if_else(dist > 0.01 & duration == 0, NA_real_,
                            duration),
         duration = if_else(speed > 100, NA_real_,
                            duration),
         duration = if_else(duration > 2 & dist > 0.31, NA_real_,
                            duration),
         duration = if_else(duration > 24, 24,
                            duration)) |>
  mutate(known_loc = if_else(dist_context <= 0.031, TRUE, FALSE)) |> 
  # subset to only known locations
  filter(known_loc == 1) |>
  select(subid, sgmnt_type, duration, dist, speed, context_id) |>
  # https://health.gov/sites/default/files/2019-09/Physical_Activity_Guidelines_2nd_edition.pdf
  # ^ walking briskly/doing chores around house ~4mph upper limit
  mutate(cut = if_else(speed > 4, 1, 0),
         row_seq = sequence(rle(as.character(context_id))$lengths))
```

This shows us that the majority of the points classified as "transit" (above 4mph) do not fall in the "middle" of being at a location.

NA points occur when speed is NaN (duration is set to NA from our previous filtering).

```{r}
first_seq_ids <- which(places_sub$row_seq == 1)
last_seq_ids <- first_seq_ids - 1
second_last_seq_ids <- first_seq_ids - 2

places_sub |>
    mutate(row_place = if_else(row_number() %in% first_seq_ids, "(1) first", "0"),
         row_place = if_else(row_number() %in% last_seq_ids, "(4) last", row_place),
         row_place = if_else(row_number() %in% second_last_seq_ids, "(3) second to last", row_place),
         row_place = if_else(row_place == "0", "(2) middle", row_place)) |> 
    tabyl(cut, row_place) |> 
    adorn_percentages("row") |> 
    adorn_pct_formatting((digits = 2)) |> 
    adorn_ns()
```

Create table with stationary versus transit filtering.
`known_loc` is NaN when `speed` is NaN, and `speed` is NaN when `dist` and
`duration` are 0.

```{r}
places |>
  mutate(duration = if_else(dist > 0.01 & duration == 0, NA_real_,
                            duration),
         duration = if_else(speed > 100, NA_real_,
                            duration),
         duration = if_else(duration > 2 & dist > 0.31, NA_real_,
                            duration),
         duration = if_else(duration > 24, 24,
                            duration),
         known_loc = if_else(dist_context <= 0.031 & speed <= 4,
                             TRUE, FALSE),
         known_loc = if_else(is.na(known_loc), FALSE, known_loc)) |>
  head(600) |>
  select(subid, lat, lon, dist, duration, speed, dist_context, known_loc) |>
  print_kbl(digits = 4) |>
  kableExtra::scroll_box(width = "600px", height = "500px")
```


### Visualizations

Visualize `dist` and `duration` scatterplot with current filtering.

```{r}
places |>
  mutate(duration = if_else(dist > 0.01 & duration == 0, NA_real_,
                            duration),
         duration = if_else(speed > 100, NA_real_,
                            duration),
         duration = if_else(duration > 2 & dist > 0.31, NA_real_,
                            duration),
         duration = if_else(duration > 24, 24,
                            duration),
         known_loc = if_else(dist_context <= 0.031 & speed <= 4,
                             TRUE, FALSE),
         known_loc = if_else(is.na(known_loc), FALSE, known_loc)) |> 
  filter(!(is.na(dist) | is.na(duration))) |>
  ggplot(aes(x = duration, y = dist)) +
  scattermore::geom_scattermore(pointsize = 3, alpha = 0.2) +
  ggmagnify::geom_magnify(from = c(xmin = 0, xmax = 2, ymin = 0, ymax = 125),
                          to = c(xmin = 10, xmax = 20, ymin = 50, ymax = 125),
                          shadow = TRUE)
```

## Final filtering decisions

Compare nrow() before and after filtering to see how many points we "lose" (AKA
how many points for which we set `duration` to NA).

```{r}
(places |> nrow()) - (places |>
                            mutate(duration = if_else(dist > 0.01 & duration == 0,
                                                      NA_real_, duration),
                                   duration = if_else(speed > 100,
                                                      NA_real_, duration),
                                   duration = if_else(duration > 2 & dist > 0.31,
                                                      NA_real_, duration),
                                   duration = if_else(duration > 24,
                                                      24, duration),
                                   known_loc = if_else(dist_context <= 0.031 & speed <= 4,
                                                       TRUE, FALSE),
                                   known_loc = if_else(is.na(known_loc),
                                                       FALSE, known_loc)) |>
                        filter(!(is.na(duration)))) |> 
  nrow()
```

Apply final filter.

```{r}
places <- places |>
  mutate(clean_duration = if_else(dist > 0.01 & duration == 0, NA_real_,
                            duration),
         clean_duration = if_else(speed > 100, NA_real_,
                            clean_duration),
         # change from 2 hours to 30 minutes
         clean_duration = if_else(clean_duration > .5 & dist > 0.31, NA_real_,
                            clean_duration),
         # removed bc many instances where this appears to be valid i.e., at home or home of friend/family
         #clean_duration = if_else(clean_duration > 24, 24,
                            #clean_duration),
         known_loc = if_else(dist_context <= 0.031 & speed <= 4,
                             TRUE, FALSE),
         known_loc = if_else(is.na(known_loc), FALSE, known_loc))
```

# EDA

Examine data following cleaning.
We will filter out rows for which `duration` is NA because these points will
not be counted in our rate calculations.

## Overview

Glimpse enriched data.

```{r}
places |>
  filter(!(is.na(duration))) |> 
  glimpse()
```

Check length of subids (verify it is the same as above).

```{r}
places |>  
  pull(subid) |>
  unique() |>  
  length()
```

Create table summarizing number of unique `context_id` observations per subject.

```{r}
places |>
  group_by(subid) |>
  summarise(unique_context = n_distinct(context_id)) |> 
  print_kbl()

places |> 
  group_by(subid) |>
  summarise(unique_context = n_distinct(context_id)) |> 
  summarise(mean(unique_context), min(unique_context), max(unique_context))
```

Display unique `context_id` observations per subject in a histogram.

```{r}
places |>
  group_by(subid) |>
  summarise(unique_context = n_distinct(context_id)) |>
  pull(unique_context) |> 
  hist(col = "white", border = "black",
       xlab = "# of Unique Context IDs", ylab = "Frequency",
       main = "Histogram of Unique Context IDs per Subject")
```

## Examine missing data

Total number of missing variables per column.

```{r}
colSums(is.na(places))
```

Review of missing data for context. Can also look at cln_locations.qmd for further exploration. Omitting `vacation` here because we will likely not be using it.

-   Type is missing for 110 for one location which accounts for all missing type in df
-   Emotion missing for one entry of subids 3, 6, and 48
-   Risk missing for an entry for subids 3, and 121.

```{r}
places[rowSums(is.na(places[, c("type", "drank", "alcohol", "emotion", "risk", "avoid")])) > 0, ] |>
  group_by(subid) |> 
  distinct(full_address, .keep_all = TRUE) |> 
  print_kbl() |> 
  kableExtra::scroll_box(width = "500px", height = "500px")
```

## Summary by context

Summary of `type` responses.

```{r}
places |> tab(type)
```

Summary of `drank` responses.

```{r}
places |> tab(drank)
```

Summary of `alcohol` responses.

```{r}
places |> tab(alcohol)
```

Summary of `emotion` responses.

```{r}
places |> tab(emotion)
```

Summary of `risk` responses.

```{r}
places |> tab(risk)
```

Summary of `avoid` responses.

```{r}
places |> tab(avoid)
```

### Calculate overall duration 

Calculate duration start to end (from first to last GPS point of a given subject on study). Compare to duration we are calculating. Then, compare to time at known locations (sum again after filtering to known locations).

Create new column to represent duration from first to last GPS point per subject. Converting this difference to hours to match with how we are currently manually calculating duration.
```{r}
places <- places |> 
  group_by(subid) |> 
  mutate(study_duration = as.numeric(difftime(last(time), first(time), units = "hours"))) |>
  ungroup()
```

Sum up duration. Duration is currently in hours.
```{r}
places <- places |> 
  group_by(subid) |> 
  mutate(sum_duration = sum(duration, na.rm = TRUE),
         sum_clean_duration = sum(clean_duration, na.rm = TRUE)) |> 
  ungroup()
```

Count number of NAs per subject for duration.
```{r}
places |>
  filter((is.na(duration))) |> 
  nrow()

places |>
  group_by(subid) |> 
  mutate(num_na_dur = sum(is.na(duration))) |> 
  select(subid, num_na_dur) |> 
  unique() |>
  ungroup()

places |>
  filter((is.na(clean_duration))) |> 
  nrow()

places |>
  group_by(subid) |> 
  mutate(num_na_dur = sum(is.na(clean_duration))) |> 
  select(subid, num_na_dur) |> 
  unique() |>
  ungroup()

```

Calculate difference between `study_duration` and `sum_duration`. A negative value means that `sum_duration` is larger, a positive value means that our `study_duration` is larger.
```{r}
places <- places |> 
  group_by(subid) |> 
  mutate(duration_diff = study_duration - sum_duration,
         clean_duration_diff = study_duration - sum_clean_duration) |> 
  ungroup()
```

Examine the differences between these two.
```{r}
places |> 
  group_by(subid) |> 
  select(subid, duration_diff, clean_duration_diff) |> 
  unique() |> 
  arrange(desc(clean_duration_diff))
```

To discuss with JC -- the majority of points lost are actually lost because of our 2hr > .31 mile jump filter. See chunk 54, there are 2389 points that are removed from that filtering which account for the majority of this time. Maybe we want to reconsider this filter?

48 * 12 = 576 hours of anticipated weekend time over the course of 3 months on study. We could consider removing subjects who have a duration discrepancy greater than 576 hours (which would be 10 subjects).

### Calculate duration for known locations

```{r}
places_known <- places |> 
  filter(known_loc == TRUE) |> 
  select(-study_duration, -duration_diff)
```

```{r}
places_known |> 
  group_by(subid) |> 
  mutate(sum_duration_known = sum(duration, na.rm = TRUE),
         perc_known = (sum_duration_known/sum_duration)*100) |> 
  select(subid, sum_duration, sum_duration_known, perc_known) |>
  unique() |> 
  arrange(desc(perc_known))
```
