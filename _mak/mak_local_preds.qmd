---
title: "Fits and evaluates best model config for `r params$version`"
author: "John Curtin & Madison, updated by Claire for study_gps"
date: "`r lubridate::today()`"
output: 
  html_document:
    toc: true 
    toc_depth: 4
format:
  html:
    embed-resources: true
params:
  study: "gps"
  version: "v9"
  cv: "kfold_5_x_5"
  algorithms: "xgboost"   # "all" or name of specific algorithm
  model: "context_movement_weather"
editor_options: 
  chunk_output_type: console
---

### Note 

Must first run `mak_best_configs.qmd` to determine best config

This script reads in CHTC performance metrics (kfold CV) and fits the best configuration across held out folds. 

Returns predictions (probabilities)

This script creates `local_preds_v*`

This script no longer calculates shaps.  That is done elswhere (`mak_shaps.qmd`)
### Set Up Environment

Function conflicts
```{r}
#| message: false
#| warning: false

library(tidyverse)
library(tidymodels)

options(conflicts.policy = "depends.ok")
library(probably)

source("https://github.com/jjcurtin/lab_support/blob/main/format_path.R?raw=true")
source("https://github.com/jjcurtin/lab_support/blob/main/fun_ml.R?raw=true")
source("https://github.com/jjcurtin/lab_support/blob/main/fun_eda.R?raw=true")
source("https://github.com/jjcurtin/lab_support/blob/main/chtc/static_files/fun_chtc.R?raw=true")

#path_input <- format_path("optimize/chtc/")
#path_models <- format_path("optimize/data_processed/models")
#path_features <-  format_path("optimize/data_processed/features")

#path_processed <- format_path(str_c("risk/data_processed/", study))
path_input <- format_path(str_c("risk/chtc/gps"))
path_models <- format_path(str_c("risk/models/gps"))
```

### Script Functions

Function to fit and predict in held out folds
```{r}
fit_predict <- function(split_num, splits, config_best){

  # write tmp file to repo to track progress through loop
  # delete this file when script is complete.  
  write_csv(tibble(stage = "eval",
                   split_num = split_num,
                   start_time = Sys.time()),
            here::here(path_models, "tmp_metrics_inner_progress"),
            append = TRUE)
  
  d_in <- splits$splits[[split_num]] |>
    training() |> 
    select(-id_obs) # not used for training; only needed in d_out to tag for later joins
  
  d_out <- splits$splits[[split_num]] |>
    testing()
    
  rec <- build_recipe(d = d_in, config = config_best)
  
  rec_prep <- rec |> 
    prep(training = d_in)
  
  feat_in <- rec_prep |> 
    bake(new_data = NULL) 
  
  model_best <- fit_best_model(config_best, feat = feat_in, "classification")

  feat_out <- rec_prep |> 
    bake(new_data = d_out) # no id_obs because not included in d_in


  # metrics from raw (uncalibrated) predictions for held out fold
  preds_prob <- predict(model_best, feat_out,
                        type = "prob")

  # Assumes we are doing stratified splits
  # train calibration model train/test split on held in data
  # Skip for baseline models
  set.seed(2468)
  cal_split <- d_in |> 
    group_initial_split(group = all_of(cv_group), 
                        strata = strat,
                        prop = 3/4)
  
  d_cal_in <- training(cal_split) 
  d_cal_out <- testing(cal_split)

  rec_cal_prep <- rec |>
    prep(training = d_cal_in)
    
  feat_cal_in <- rec_cal_prep |> 
    bake(new_data = NULL) 

  feat_cal_out <- rec_cal_prep |>  
    bake(new_data = d_cal_out) 

  model_cal <- fit_best_model(config_best, feat = feat_cal_in, "classification")

  # iso calibration
  iso <- predict(model_cal, feat_cal_out,
                 type = "prob") |> 
    mutate(truth = feat_cal_out$y) |> 
    cal_estimate_isotonic(truth = truth,
                          estimate = dplyr::starts_with(".pred_"))
  
  preds_prob_iso <- preds_prob |> 
    cal_apply(iso)

  # logistic calibration
  logi <- predict(model_cal, feat_cal_out,
                 type = "prob") |>
    mutate(truth = feat_cal_out$y) |>
    cal_estimate_logistic(truth = truth,
                           estimate = dplyr::starts_with(".pred_"),
                           smooth = TRUE)
  
  preds_prob_logi <- preds_prob |>
    cal_apply(logi)

  # beta calibration
  beta <- predict(model_cal, feat_cal_out,
                  type = "prob") |>
    mutate(truth = feat_cal_out$y) |>
    cal_estimate_beta(truth = truth,
                      estimate = dplyr::starts_with(".pred_"))
  
  preds_prob_beta <- preds_prob |>
    cal_apply(beta)

  # combine raw and calibrated probs (with id_obs and label)
  tibble(id_obs = d_out$id_obs,
         split_num = rep(split_num, nrow(preds_prob)),
         prob_raw = preds_prob[[str_c(".pred_", y_level_pos)]],
         prob_iso = preds_prob_iso[[str_c(".pred_", y_level_pos)]],
         prob_logi = preds_prob_logi[[str_c(".pred_", y_level_pos)]],
         #prob_beta = preds_prob_beta[[str_c(".pred_", y_level_pos)]],
         ## make able to handle NAs generated above
         prob_beta = if (is.data.frame(preds_prob_beta)) {
           preds_prob_beta[[str_c(".pred_", y_level_pos)]]
           } else {
             rep(NA_real_, nrow(preds_prob))
             },
         label = d_out$y) 
}
```


### Read in aggregate CHTC metrics 
```{r}
config_best <- 
  read_csv(here::here(path_models, 
                      str_c("best_config_",
                            params$version, "_",
                            params$cv, "_",
                            params$model, ".csv"))) |>
  mutate(n_folds = n(), # think we need this check with john
         algorithm = "xgboost") |> 
  slice(1) |> 
  select(feature_set, hp1, hp2, hp3, resample, n_folds, split_num, algorithm) |>
  #mutate(algorithm = "xgboost") |>
  glimpse()
```

### Fit best model from  CHTC CV and get/save preds

- Assumes full recipe is for all algorithms is present in all training controls with branches/ifs to select proper algorithm specific steps

- Map over all splits to get predicted probabilities from held out fold. Then save predicted probs

```{r} 
# can source any training control given assumptions above
batch_names <- list.dirs(path_input, full.names = FALSE, recursive = FALSE)
batch_name <- batch_names[str_detect(batch_names, params$version)]
path_batch <- here::here(path_input, batch_name)
# NOTE: training controls overwrites path_batch but it matches   
source(here::here(path_batch, "input", "training_controls.R"))
          
chunks <- str_split_fixed(data_trn, "\\.", n = Inf) # parse name from extensions
if (length(chunks) == 2) {
  fn <- str_c("data_trn.", chunks[[2]])
} else {
  fn <- str_c("data_trn.", chunks[[2]], ".", chunks[[3]])
}
  
# open based on file type
if (str_detect(fn, "csv")) {
  d <- read_csv(here::here(path_batch, "input", fn), show_col_types = FALSE) 
} else {
  d <- read_rds(here::here(path_batch, "input", fn))
}

d <- d |> 
  arrange(subid, dttm_label) |> 
  format_data() |> 
  mutate(id_obs = 1:nrow(d))  # tmp add for linking obs.  Removed in fit_predict()

# assumes using stratification on cv_strat
splits <- d |> 
  make_splits(cv_resample_type, cv_resample, cv_outer_resample, 
              cv_inner_resample, cv_group, cv_strat = cv_strat,
              the_seed = seed_splits)

all <- 1:config_best$n_folds |> # is n_folds the right number to put here?
  map(\(split_num) fit_predict(split_num = split_num, 
                                     splits = splits, 
                                     config_best = config_best)) |> 
  list_rbind() |> 
  write_csv(here::here(path_models, str_c("local_preds_", 
                                         params$version, ".csv")))

# delete tracking file
if(file.exists(here::here(path_models, "tmp_metrics_inner_progress"))) {
  file.remove(here::here(path_models, "tmp_metrics_inner_progress"))
}
```