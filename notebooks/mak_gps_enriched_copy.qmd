---
title: "Make enriched gps/location"
author: "John Curtin, updated by Claire Punturieri"
date: "`r lubridate::today()`"
format: 
  html:
    toc: true
    toc_depth: 4
    embed-resources: true
editor_options: 
  chunk_output_type: console
---

# Housekeeping

## Code Status

Updated to current lab practices 07/2024.

## Notes   

This script aggregates GPS files for all subjects and then matches each 
geolocation to its nearest context.

The sample starts with the N = 151 participants who completed through at 
least follow-up 1 and who had valid lapse data.

We then did EDA (gps_eda.qmd) to confirm that GPS data are valid for these N=151.  
This resulted in further removals. Final sample is N = 146, with some subjects
having their data truncated to shorter periods of good data.

More details on these decisions are available in the cleaning of EMA and lapses
and in the mak_study_dates.qmd script.

# Create and clean enriched data

## Set up

### Set up environment
```{r}
#| message: false
#| warning: false

options(conflicts.policy = "depends.ok")
devtools::source_url("https://github.com/jjcurtin/lab_support/blob/main/format_path.R?raw=true", 
                      sha1 = "a58e57da996d1b70bb9a5b58241325d6fd78890f")
```

### Paths
```{r}
path_shared <- format_path("studydata/risk/data_processed/shared")
path_gps <- format_path("studydata/risk/data_processed/gps")
```

### Packages and source
```{r}
#| message: false
#| warning: false

# for data wrangling
library(tidyverse)
library(lubridate)

source(here::here("../lab_support/fun_gps.R"))
```

### Initialize parallel processing
```{r}
cl <- parallel::makePSOCKcluster(parallel::detectCores(logical = FALSE))
doParallel::registerDoParallel(cl)
```


## Data wrangling

### Study Dates

Open study dates b/c it contains all the subids with context and valid lapses and 
start and end dates for good GPS (determined iteratively in this script).

```{r}
study_dates <- read_csv(here::here(path_gps, "study_dates.csv"), col_types = "dTTT",
                     show_col_types = FALSE) |>  
  mutate(study_start = as_date(study_start),
         study_end = as_date(study_end))  |>  
  mutate(study_ndays = as.numeric(difftime(study_end, study_start, units = "days")))  |>  
  glimpse()
```

Extract subids from study_dates file.
```{r}
subids_dates <- study_dates |>  
  pull(subid) |>  
  unique()
```

### Locations (context)

Load in contextual information file.
```{r}
locs <- read_csv(here::here(path_shared, "locations.csv"), col_types = "cddcccccccccccdd",
              show_col_types = FALSE) |>  
  mutate(subid = as.numeric(subid)) |>  
  glimpse()
```

Extract subids from locations file.
```{r}
subids_locs <- locs |>  
  pull(subid) |>  
  unique()
```

Check if there are any subids missing location context.
```{r}
if(any(!(subids_dates %in% subids_locs))) stop("Missing location for some subids")
```

### GPS

Load in GPS data, only for verified subjects (those in study_dates file).

> CP 7/9: think we should also filter on study_dates here?

```{r}
gps <-  read_csv(here::here(path_shared, "gps.csv"), col_types = "ddTdccccdddd",
              show_col_types = FALSE)  |>  
  filter(subid %in% subids_dates)  |>    # limit to sample in study_dates
  mutate(time = with_tz(time, tz = "America/Chicago")) |>  
  group_by(subid)  |>  
  mutate(dist = distGeo(as.matrix(tibble(lon, lat))),
         duration = difftime(lead(time), time, units = "mins"),
         duration = as.numeric(duration),
         duration = if_else(duration > 5 & dist > 500, NA_real_, duration)) |>  # set suspect gaps to NA if high duration AND moved
  ungroup()  |>  
  glimpse()
```

Extract subids from GPS file.
```{r}
subids_gps <- gps |>  
  pull(subid)  |>  
  unique()  |>  
  print()
```

Check if there are any subids missing GPS.
```{r}
if(any(!(subids_dates %in% subids_gps))) stop("Missing gps for some subids")
```

### Merge context into GPS

Create function to enrich GPS with context for one subid
```{r}
enrich_subid <- function(a_subid, gps, locs) {
  gps <- gps |>  
    filter(subid == a_subid)
  
  locs <- locs |>  
    filter(subid == a_subid)
  
  enriched <- gps |>  
    bind_cols(map2_dfr(gps$lon, gps$lat, find_nearest_context, context = locs)) 
  
  return(enriched)
}
```

future_map over subids
```{r}
enriched <- subids_gps  |>  
  future_map_dfr(enrich_subid, gps = gps, locs = locs)  |>  # check if up to date
  left_join(locs, by = c("subid", "context_id"))  |>  
  select(-lat.y, -lon.y, -accuracy, -speed_kmh, -altitude_meters, -direction,
         -data_type)  |>  
  rename(lat = lat.x, lon = lon.x)  |>  
  relocate(subid)
```


#  Brief EDA

```{r}
enriched  |>   
  glimpse()

enriched  |>  
  pull(subid)  |>
  unique() |>  
  length()
```

```{r}
enriched |>  
  tabyl(subid) |>  
  arrange(n)

enriched |>  
  tabyl(subid) |> 
  summarize(min(n), max(n))

enriched |>  
  tabyl(subid) |> 
  pull(n) |>  
  hist()

enriched |>  
  tabyl(subid) |> 
  filter(n < 5000) |> 
  pull(n) |>  
  hist()
```

> CP 6/12: do we still need this section below?? Examining missing data already done in cln_locations.

Review of missing data for context

* type is missing for 110 for one location which accounts for all missing type in df
* emotion missing for one entry ofr subids 3, 6, and 48
* risk missing for an entry for subids 3, and 121.
```{r}
enriched  |>  
  summarize(across(.fns = ~sum(is.na(.))))  |>  
  glimpse()
```

```{r}
enriched  |>  tabyl(type)
enriched  |>  tabyl(drank)
enriched  |>  tabyl(alcohol)
enriched  |>  tabyl(emotion)
enriched  |>  tabyl(risk)
enriched  |>  tabyl(avoid)
enriched  |>  tabyl(vacation)
```

### EDA for participant removal based on missing days

```{r}
enriched <- enriched  |>  
  mutate(time = with_tz(time, tzone = "America/Chicago")) |>  # done here again to be safe before convert to date
  mutate(gps_date = as_date(time))  |> 
  left_join(select(study_dates, subid, study_start, study_end, study_ndays), by = "subid")  |>  
  filter(gps_date >= study_start,
         gps_date <= study_end)  |>  
  glimpse()


# number of days with gps points
enriched  |>  
  group_by(subid, gps_date)  |>  
  summarize(n())  |>  
  group_by(subid)  |>  
  summarize(gps_ndays = n()) |> 
  left_join(select(study_dates, subid, study_ndays), by = "subid")  |>  
  mutate(days_diff = gps_ndays - study_ndays)  |>  
  arrange(days_diff)  |>  
  print(n = Inf)  |>  
  pull(days_diff)  |>  
  hist(main = "Days of Missing GPS")
```

# Save enriched GPS

Saving xz compressed for use on CHTC for feature engineering

```{r}
enriched  |>  
  vroom::vroom_write(here::here(path_gps, "gps_enriched.csv.xz"), delim = ",")
```