---
title: "Make enriched gps/location"
author: "John Curtin, updated by Claire Punturieri"
date: "`r lubridate::today()`"
format: 
  html:
    toc: true
    toc_depth: 4
    embed-resources: true
editor_options: 
  chunk_output_type: console
---

# Housekeeping

## Code Status

In progress as of 07/2024.

## Notes   

This script does EDA on the enriched GPS file, generated via mak_gps_enriched.qmd

# Set up

## Set up environment
```{r}
#| message: false
#| warning: false

options(conflicts.policy = "depends.ok")
devtools::source_url("https://github.com/jjcurtin/lab_support/blob/main/format_path.R?raw=true", 
                      sha1 = "a58e57da996d1b70bb9a5b58241325d6fd78890f")
```

## Paths
```{r}
path_shared <- format_path("studydata/risk/data_processed/shared")
path_gps <- format_path("studydata/risk/data_processed/gps")
```

## Packages and source
```{r}
#| message: false
#| warning: false

# for data wrangling
library(tidyverse)

# helpful lab functions
devtools::source_url("https://github.com/jjcurtin/lab_support/blob/main/fun_eda.R?raw=true",
 sha1 = "c045eee2655a18dc85e715b78182f176327358a7"
)

# adjust plot visuals
theme_set(theme_classic())
options(tibble.width = Inf, dplyr.print_max = Inf)
```

## Load in data
```{r}
enriched <- read_csv(here::here(path_gps, "gps_enriched.csv.xz"), 
                 show_col_types = FALSE)
```

# EDA

## Overview

Glimpse enriched data.
```{r}
enriched  |>   
  glimpse()
```

Check length of subids (verify it is the same as above).
```{r}
enriched  |>  
  pull(subid) |>
  unique() |>  
  length()
```

Create table summarizing number of unique `context_id` observations per subject.
```{r}
enriched |>
  group_by(subid) |>
  summarise(unique_context = n_distinct(context_id)) |> 
  print_kbl()

enriched |> 
  group_by(subid) |>
  summarise(unique_context = n_distinct(context_id)) |> 
  summarise(mean(unique_context), min(unique_context), max(unique_context))
```

Display unique `context_id` observations per subject in a histogram.
```{r}
enriched |>
  group_by(subid) |>
  summarise(unique_context = n_distinct(context_id)) |>
  pull(unique_context) |> 
  hist(col = "white", border = "black",
       xlab = "# of Unique Context IDs", ylab = "Frequency",
       main = "Histogram of Unique Context IDs per Subject")
```

## Test filtering

Filtering
```{r}
enriched <- enriched |> 
  filter(!(duration > 5 & dist > 500)) |> # filter out improbable jumps
         # calculate speed in kilometers per hour
  mutate(speed = (dist / 1000)/(duration / 60), # change to mph?
         # dist = 0 and duration = 0 will lead to NaN response, set to 0
         # not sure this is the best course of action?
         # remove duration 0 points? set automatically to be transit?
         # CLAIRE CHECK THIS
         # remove missing values if confirmed same point
         ##speed = if_else(is.na(speed), 0, speed), 
         # assume at known location if within 50m
         # set to T/F
         known_loc = if_else(dist_context <= 50, 1, 0),
         # set points at or below 1 km/h to be stationary
         track_type = if_else(speed <= 1, "stationary", "transit")) |> 
  filter(!(is.na(speed)))
```

Percentage of points above ~100m/min (play around with), to see if they are
at the beginning or end of a series (want most points to fall in middle).
```{r}
enriched |> filter(known_loc == 1) |> select(subid, sgmnt_type, date, dist, context_id) |> View()

enriched |> filter(known_loc == 1) |> select(subid, sgmnt_type, track_type, duration, dist, speed, context_id) |>
  # reverse to filter out
  mutate(cut = if_else(speed < 5, 1,0)) |> pull(cut) |> mean()
```

Examine points with duration longer than forced sampling time.
```{r}
enriched |>
  # sampling should be forced every 2 hours, remove duration above that?
  # interpolation?
  filter(duration > 121) |> 
  select(subid, dist, duration) |> 
  arrange(desc(dist)) |> 
  print_kbl()
```

Look at breakdown of stationary versus transit points.
```{r}
enriched |> tab(track_type)
```

Compare calculated track_type with Moves calculated trckpnt_type.
```{r}
enriched |>
  filter(app_source == "moves") |> 
  tab2(trckpnt_type, track_type)
  select(subid, trckpnt_type, track_type, speed) |> 
  print_kbl()
```

Look at speed in relation to known locations.
```{r}
enriched |>
  filter(known_loc == 1) |>
  select(subid, context_id, dist, duration, speed, track_type) |> print_kbl()
```

Look at breakdown of states -- not sure if it's worth filtering based on state?
Thought this would make a difference but I'm not so sure if we standardize features.
```{r}
enriched |> tab(state) |> arrange(desc(n))
```

Look at number of "other" points.
```{r}

```

## Distance & duration

```{r}
enriched |> 
  filter((duration > 5 & dist > 500)) |> 
  pull(dist) |>
  median()
  hist()
  print_kbl()

```

Look at spread of duration.
```{r}
enriched |>
  filter(duration < 15) |> 
  ggplot(aes(x = duration)) +
  geom_histogram(color = "black", fill = "white") +
  xlab("Time since last point (< 15min)")
```

Look at spread of context distance.
```{r}
enriched |>
  filter(dist_context < 500) |> 
  ggplot(aes(x = dist_context)) +
  geom_histogram(color = "black", fill = "white") +
  xlab("Distance from closest known context (< 500m)")
```

### Distance & duration around 1-observation days

Define function to pull days with one GPS observation and the preceding and
following row.
```{r}
single_obs <- function(enriched_obs) {
  inds <- which(enriched_obs$n_obs == 1)
  rows <- lapply(inds, function(x) (x-1):(x+1))
  enriched_obs[unlist(rows),]
}
```

Count number of observations per day, then add enriched file data in.
```{r}
enriched_obs <- enriched |>
  group_by(subid, date) |>  
  summarise(n_obs = n())

enriched_obs <- enriched_obs |> 
  right_join(enriched, by = c("subid", "date"))
```

Print table displaying 1-observation days, and preceding/subsequent rows.
```{r}
enriched_obs |>
  single_obs() |>
  select(subid, date, n_obs, duration, dist) |>
  print_kbl()
```

## Examine missing data

Total number of missing variables per column.
```{r}
colSums(is.na(enriched))
```

Review of missing data for context. Can also look at cln_locations.qmd for further exploration. 
Omitting `vacation` here because we will likely not be using it.

- Type is missing for 110 for one location which accounts for all missing type in df
- Emotion missing for one entry of subids 3, 6, and 48
- Risk missing for an entry for subids 3, and 121.
```{r}
enriched[rowSums(is.na(enriched[, c("type", "drank", "alcohol", "emotion", "risk", "avoid")])) > 0, ] |>
  group_by(subid) |> 
  distinct(full_address, .keep_all = TRUE) |> 
  print_kbl()
```

## Summary by context

Summary of `type` responses.
```{r}
enriched  |>  tab(type)
```

```{r}
enriched |>
  group_by(subid) |>
  distinct(context_id, .keep_all = TRUE) |> 
  filter(type == "other") |>
  select(-sgmnt_type, -trckpnt_type, -app_source,
         -lat_context, -lon_context, -dist_context, -utc,
         -dist, -duration) |> 
  print_kbl()
```

Summary of `drank` responses.
```{r}
enriched  |>  tab(drank)
```

Summary of `alcohol` responses.
```{r}
enriched  |>  tab(alcohol)
```

Summary of `emotion` responses.
```{r}
enriched  |>  tab(emotion)
```

Summary of `risk` responses.
```{r}
enriched  |>  tab(risk)
```

Summary of `avoid` responses.
```{r}
enriched  |>  tab(avoid)
```

## Test feature development

Homestay test
```{r}
homestay <- enriched_test |> filter(subid == 1) |>
  filter(known_loc == 1) |> 
  filter(type == "home") |> pull(duration) |> sum()

totalstay <- enriched_test |> filter(subid == 1) |> 
  pull(duration) |> sum()

homestay/totalstay * 100
```

Location variance test
```{r}
latvar <- enriched_test |> filter(subid == 1) |> 
  pull(lat) |> var()

lonvar <- enriched_test |> filter(subid == 1) |> 
  pull(lon) |> var()

log(latvar + lonvar)
```
