---
title: "Processes `r params$model` models of training jobs from CHTC for `r params$study` study with `r params$window` window and `r params$lead` lead and version `r params$version` using `r params$cv` CV"
author: "John Curtin & Kendra Wyant"
date: "`r lubridate::today()`"
output: 
  html_document:
    toc: true 
    toc_depth: 4
format:
  html:
    embed-resources: true
params:
  study: "gps"
  version: "v5"
  cv: "nested_1_x_10_3_x_10"
  algorithms: "xgboost"   # "all" or name of specific algorithm
  model: "main"
editor_options: 
  chunk_output_type: console
---

### Code Status

edit title

In use for both kfold and nested cv, including use of batches

This script aggregates all results/metrics for a batch or batches of jobs that train all model configurations for a specific outcome/label window.

### Set Up Environment

```{r}
study <- params$study
version <- params$version
cv <- params$cv
algorithms <- params$algorithms
model <- params$model
```

Handle conflicts

```{r}
#| message: false
#| warning: false

options(conflicts.policy = "depends.ok")
```

Packages for script

```{r}
#| message: false
#| warning: false

library(tidyverse)

devtools::source_url("https://github.com/jjcurtin/lab_support/blob/main/fun_eda.R?raw=true")
theme_set(theme_classic()) 
```

Absolute paths

```{r}
#| message: false

source("https://github.com/jjcurtin/lab_support/blob/main/format_path.R?raw=true")

path_input <- format_path(str_c("studydata/risk/chtc/", study))
path_models <- format_path(str_c("studydata/risk/models/", study))
path_output <- format_path(str_c("studydata/risk/chtc/", study))

```

Chunk Defaults

```{r}
#| include: false

knitr::opts_chunk$set(attr.output='style="max-height: 500px;"')

options(tibble.width = Inf)
options(tibble.print_max = Inf)
```

### Read results.csv files

CP: commented this part out for now

Set up object for results

```{r}
# results_all <- NULL
```

Get batch_names

```{r}
batch_name <- str_c("train_", algorithms, "_", cv, "_", version, "_", model)
# batch_names <- list.dirs(path_input, full.names = FALSE, recursive = FALSE)
#   
# if (algorithms == "all") {
#   batch_names <- batch_names[str_detect(batch_names, "train") & 
#                                str_detect(batch_names, cv) &
#                                str_detect(batch_names, version) &
#                                str_detect(batch_names, window) &
#                                str_detect(batch_names, 
#                                           str_c(as.character(lead), "lag"))]
#   if (model == "main") {
#     batch_names <- batch_names[!str_detect(batch_names, "baseline")]
#   }
# } else {
#     batch_names <- batch_names[str_detect(batch_names, "train") & 
#                                str_detect(batch_names, cv) &
#                                str_detect(batch_names, version) &
#                                str_detect(batch_names, window) &
#                                 str_detect(batch_names, 
#                                           str_c(as.character(lead), "lag")) &
#                                str_detect(batch_names, algorithms)]
#   if (model == "main") {
#     batch_names <- batch_names[!str_detect(batch_names, "baseline")]
#   }
# }  
#  
# batch_names
```

Loop over batch_names to read in files and perform checks

```{r}
# 
# for (batch_name in batch_names) {
#   message("Processing Batch: ", batch_name)
#   
#   # read in configs
#   configs <- read_csv(here::here(path_input, batch_name, "input", "configs.csv"), 
#                       show_col_types = FALSE)
#   (n_configs <- nrow(configs))
#   
#   # read in results
#   results_batch <- read_csv(here::here(path_input, batch_name, "output", 
#                                       "batch_results.csv"), 
#                             show_col_types = FALSE)
#   (n_results_batch <- nrow(results_batch))
#   
#   # Check counts of results files
#   if (str_detect(batch_name, "xgboost")) {
#     source(here::here(path_input, batch_name, "input/training_controls.R"))
#     check <- (n_configs) == (n_results_batch)/hp2_glmnet_out
#   } else {
#     check <- n_configs == n_results_batch
#   }  
#   if (!check) {
#     stop(n_configs, " configs != ", n_results_batch, " results files!")
#   } else {
#     message(n_results_batch, " results files detected.  Correct!")
#   }
#   
#   # Check col count
#   if (!(ncol(results_batch) == 17)) {
#     stop(ncol(results_batch), " columns != 17")
#   } else {
#     message(ncol(results_batch), " columns detected.  Correct!\n")
#   }
#   
#   # results_batch %>% tab(split_num) %>% print()
#   # results_batch %>% tab(outer_split_num) %>% print()
#   # results_batch %>% tab(inner_split_num) %>% print()
#   # results_batch %>% tab(algorithm) %>% print()
#   # results_batch %>% tab(feature_set) %>% print()
#   # results_batch %>% tab(hp1) %>% print()
#   # results_batch %>% tab(hp2) %>% print()
#   # results_batch %>% tab(hp3) %>% print()
#   # results_batch %>% tab(resample) %>% print()
# 
#   # Add batch to all metrics
#   results_all <- results_all %>% 
#     bind_rows(results_batch)
# }
```

### Wrap up processing of raw metrics

Load in results file
```{r}
results_all <- read_csv(here::here(path_input, batch_name, "output", "batch_results.csv"),
                        show_col_types = FALSE)
```

Remove duplicate rows (e.g., same hyper-parameters across multiple batches)

```{r}
# nrow(results_all)
# 
# results_all <- results_all |> 
#   distinct(split_num, outer_split_num, inner_split_num, algorithm, feature_set,
#            hp1, hp2, hp3, resample, .keep_all = TRUE)
# 
# nrow(results_all)
```

Final checks across all batches

```{r}
  results_all %>% tab(split_num) %>% print()
  results_all %>% tab(outer_split_num) %>% print()
  results_all %>% tab(inner_split_num) %>% print()
  results_all %>% tab(algorithm) %>% print()
  results_all %>% tab(feature_set) %>% print()
  results_all %>% tab(hp1) %>% print()
  results_all %>% tab(hp2) %>% print()
  results_all %>% tab(hp3) %>% print()
  results_all %>% tab(resample) %>% print()
```

Save raw metrics file

```{r}
results_all %>%
  arrange(split_num, outer_split_num, inner_split_num, algorithm, resample) |> 
  write_csv(here::here(path_models, str_c("inner_metrics_",
                                             version, "_",
                                             cv, "_", model, ".csv")))
```

### Median metrics across inner folds for model configurations

Inner loop performance of best config.
This median performance for each configuration over inner x outer folds (e.g., 300 folds for 1x10 inner and 3x10 outer). It is what we would get (essentially) if we just did simple k-fold but with LOTs of folds

```{r}
metrics_avg <- results_all %>% 
  group_by(algorithm, feature_set, hp1, hp2, hp3, resample) %>% 
   summarize(median_roc_auc = median(roc_auc),
             n_folds = n(), .groups = "drop") %>% 
  relocate(n_folds) %>% 
  arrange(desc(median_roc_auc)) |> 
  ungroup()

metrics_avg |> 
  slice(1:50) |> 
  print_kbl()

best_config <- metrics_avg |> 
  slice(1) |> 
  print()
```

Performance metric plot across all inner folds

```{r}
results_all |> 
  filter(algorithm == best_config$algorithm,
         feature_set == best_config$feature_set,
         hp1 == best_config$hp1,
         hp2 == best_config$hp2,
         hp3 == best_config$hp3,
         resample == best_config$resample) |> 
  ggplot(aes(x = roc_auc)) +
  geom_histogram(bins = 10)
```

